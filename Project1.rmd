---
title: "Computer Intensive Statistical Methods-Exercise 1"
author: "Vebjørn Rekkebo, Camilla Karlsen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

The inverse of the exponential cumulative density function with rate parameter $\lambda$ is given by 
$$
F^{-1}(u) = -\frac{1}{\lambda}\text{log}(u).
$$
Hence we can sample from the exponential distribution by generating $u\sim \mathcal{U}[0,1]$ and returning $-\frac{1}{\lambda}\text{log}(u)$.
```{r, include=TRUE}
#This function returns a vector of n independent samples from the exponential distribution with rate parameter lam, generated from the inverse cumulative distribution. 
random.exp <- function(n,lam){
  return(-1/lam * log(runif(n)))
}
```
We know that $\text{E}(X)=\frac{1}{\lambda}$ and $\text{Var}(X)=\frac{1}{\lambda^2}$. The empirical mean is given by the average of the sample and the empirical variance is $\widehat{\text{Var}(X)}=\frac{\sum_{i=1}^n x_i}{n-1}$.
```{r}
#Testing function
test.function <- function(n,func,arg,amean,avar){
  exs.sample=func(n,arg)
  cat("Empirical mean:", mean(exs.sample), "Analytical mean:",amean, "\n")
  cat("Empirical variance:", var(exs.sample), "Analytical variance:",avar,"\n")
}

lam <- 3
sample.exp <- enframe(random.exp(100000,lam))

ggplot() +
  geom_histogram(
    data = data.frame(sample.exp),
    mapping = aes(x=value, y=..density..),
    binwidth = 0.01
  ) +
  geom_vline(
    aes(xintercept=mean(sample.exp$value),
        col='Empirical mean')
  ) +
  stat_function(
    fun = dexp,
    args = list(rate=lam),
    aes(col='Theoretical density')
  ) +
  labs(
    caption = 'Histogram'
  )


test.function(10000,random.exp,lam,1/lam,1/lam^2)

```

The printout shows that the empirical mean and variance are close to the analytical values so the implementation seems correct.

We now consider the density function 
$$
g(x) = \begin{cases} cx^{\alpha-1}, &0<x<1,\\ ce^{-x}, &1\leq x, \\ 0, &\text{otherwise,} \end{cases}
$$
where c is a normalising constant and $\alpha \in (0,1)$. Integration over the different intervals yields the cumulative distribution 
$$
G(x) = \begin{cases} \frac{c}{\alpha}x^\alpha, &0<x<1,\\ c(\alpha^{-1}+e^{-1}-e^{-x}) &1\leq x, \\ 0, &\text{otherwise}. \end{cases}
$$
We can now set $u=G(x)$ and solve for $x$
$$\begin{cases} 
u=\frac{c}{\alpha}x^\alpha &\iff
x = (\frac{\alpha}{c}u)^{\frac{1}{\alpha}} , &0 < x < 1, \\
u = c(\alpha^{-1}+e^{-1}-e^{-x}) &\iff x=-\text{log}(\alpha^{-1}+e^{-1}-\frac{u}{c}), &1\leq x,
\end{cases}
$$
and the boundary $x=1$ is equivalent to $u=\frac{c}{\alpha}$. Hence the inverse of the cumulative distribution is
$$
G^{-1}(u) = 
\begin{cases}
(\frac{\alpha}{c}u)^{\frac{1}{\alpha}} , &0 < u < \frac{c}{\alpha}, \\
-\text{log}(\alpha^{-1}+e^{-1}-e^{-x}), &\frac{c}{\alpha} \leq u, \\
0, &\text{otherwise}.
\end{cases}
$$
The normalising constant $c$ is found by integrating $g(x)$ over its domain and let the integral equal 1.
$$
\int_0^\infty g(x)dx = c(\alpha^{-1}+e^{-1})=1 \iff c =\frac{\alpha e}{\alpha+e}.
$$
Now we can sample from $g(x)$ by generating $u\sim \mathcal{U}[0,1]$ and returning $G^{-1}(u)$.

```{r, include=TRUE}

#Function for normalising constant in g distribution given parameter a (alpha).
c.g <- function(a){
  return (a*exp(1)/(a+exp(1)))
}

g <- function(x, a) {
  c <- c.g(a)
  out <- rep(0, length(x))
  left <- 0 < x & x < 1
  right <- 1 <= x
  out[left] <- c * (x[left] ^ (a - 1))
  out[right] <- c * exp(-x[right])
return(out)
}

#This function returns a vector of n independent samples from the g distribution with parameter a (=alpha), generated from the inverse cumulative distribution. 
random.g <- function(n,a){
  c <- c.g(a) #Normalising constant
  u <- runif(n)
  x <- rep(0,n)
  for (i in 1:n){
    if (u[i]<c/a){
      x[i] <- (a*u[i]/c)^(1/a)
    }
    else{
      x[i] <- (-log(1/a + exp(-1) - u[i]/c))
    }
  }
  return(x)
}

```
The analytical mean and variance of $X\sim g(x)$ are computed in the usual way 
\begin{align}
\text{E}(X) &=\int_0^\infty xg(x)dx=\frac{c}{\alpha+1}+\frac{2c}{e}, \\
\text{E}(X^2) &=\int_0^\infty x^2g(x)dx=\frac{c}{\alpha+2}+\frac{5c}{e}, \\
\text{Var}(X) &=\text{E}(X^2)-(\text{E}(X))^2 = c(\frac{1}{\alpha+2}+\frac{5}{e})-(\frac{c}{\alpha+1}+\frac{2c}{e})^2. 
\end{align}

```{r, include=TRUE}

#Testing funtion
a <- .6
c <- c.g(a)
g.mean <- c/(a+1)+2*c*exp(-1) #analytical mean
g.var <- c*(1/(a+2)+ 5*exp(-1)) - g.mean^2 #analytical variance
# test.function(10000,random.g,a,g.mean,g.var)
sample.g <- enframe(random.g(100000,a))

ggplot() +
  geom_histogram(
    data = data.frame(sample.g),
    mapping = aes(x=value, y=..density..),
    binwidth = 0.01,
    na.rm = FALSE
  ) +
  geom_vline(
    aes(xintercept=mean(sample.g$value),
        col='Empirical mean')
  ) +
  stat_function(
    fun = g,
    args = list(a=a),
    aes(col='Theoretical density')
  ) +
  ylim(0,2) +
  xlim(0,5)
```

We can see that analytical and empirical moments coincide.

The Box-Müller algorithm samples from the bivariate standard normal distribution. We first generate $x_1 \sim \mathcal{U}(0,2\pi)$ and $x_2 \sim \text{exp}(\frac{1}{2})$ and then return $y_1=\sqrt{x_2}\text{cos}(x_1)$ and $y_2=\sqrt{x_2}\text{sin}(x_1)$. We want to generate a vector of $n$ independent samples from the standard normal distribution. Thus we only keep $y_1$ from each bivariate sample.

```{r, include=TRUE}
#This function returns a vector of n independent samples from the standard normal distribution, generated by the Box-Müller algorithm
random.norm <- function(n){
  x1 <- runif(n)*2*pi
  x2 <- random.exp(n,.5)
  return(sqrt(x2)*cos(x1))
}

#Testing function
sample.normal=enframe(random.norm(100000))
cat("Empirical mean:", mean(sample.normal), "Analytical mean:",0.0, "\n")
cat("Empirical variance:", var(sample.normal), "Analytical variance:",1.0,"\n")

ggplot() +
  geom_histogram(
    data = data.frame(sample.normal),
    mapping = aes(x=value, y=..density..),
    binwidth = 0.01
  ) +
  geom_vline(
    aes(xintercept=mean(sample.normal$value),
        col='Empirical mean')
  ) +
  stat_function(
    fun = dnorm,
    aes(col='Theoretical density')
  )

```

The empirical mean and variance are close to the analytical values so the implementation seems correct.

Now we want to sample from a d-variate normal distribution with mean vector $\boldsymbol{\mu}$ and covariance matrix $\mathbf{\Sigma}$. This is done by generating a vector $\boldsymbol{x}$ of $d$ standard normal samples and doing the transform $\boldsymbol{y}=\mathbf{A}\boldsymbol{x}+\boldsymbol{\mu}$ where $\mathbf{AA^T=\Sigma}$. We find $\mathbf{A}$ by the cholesky decomposition of $\mathbf{\Sigma}$.

```{r}
#This function returns a sample from the d-variate normal distribution with mean mu and covariance S, transformed from standard normal samples. 
random.multinorm <- function(d,mu,S){
  x <- random.norm(d)
  A <- t(chol(S)) #transpose to get lower triangular cholesky matrix
  return(mu + A%*%x)
}

#Testing function
d = 3
N = 10000
sigma <- cbind(c(3,1,0), c(1,3,1), c(0,1,3)) #if d=3
# sigma <- cbind(c(2,1),c(1,2))                  #if d=2
mu <- seq(1,d)
multinormal.sample <- matrix(NA,N,d)
for (i in 1:N){
  multinormal.sample[i,] <- random.multinorm(d, mu, sigma)
}
mu
colMeans(multinormal.sample)
sigma
var(multinormal.sample)
```


# Problem B: The gamma distribution
First, we consider the gamma distribution with parameters $\alpha \in (0,1)$ and $\beta=1$. To generate samples from $f(x)$, we use rejection sampling with $g(x)$ as proposal density. We need to find a $k>1$ such that $k\geq\frac{f(x)}{g(x)}$.
$$
\frac{f(x)}{g(x)}=\begin{cases}\frac{1}{c\Gamma(\alpha)}e^{-x}, & 0<x<1 \\ \frac{1}{c\Gamma(\alpha)}x^{\alpha -1}, & 1 \leq x \end{cases}\leq \frac{1}{c\Gamma(\alpha)}=k
$$
Hence, the acceptance probability in the rejection sampling is

$$
\alpha=\frac{1}{k}\frac{f(x)}{g(x)}=c\Gamma(\alpha)\frac{f(x)}{g(x)}=\begin{cases}e^{-x}, & 0<x<1 \\ x^{\alpha -1}, & 1 \leq x \end{cases}.
$$

```{r}
#Acceptance probability
accept_prob <- function(x,a){ 
  if (x<1)
      return (exp(-x))
    else 
      return (x^(a-1))
}
```

```{r, include=TRUE}
#This function returns a vector of n independent samples from the gamma distribution with 0<a<1, beta=1, generated by the Rejection sampling
random.gamma1 <- function(n,a){ 
  x=rep(0,n)
  for (i in 1:n){
    finish = 0
    while(finish==0){  
      # Sample from the proposal distribution, g(x) (x>=0)
      sample.x = random.g(1,a)
      # Compute the acceptance probability alpha
      alpha=accept_prob(sample.x,a)
      # Generate a helping variable U from a uniform(0,1)
      u = runif(1, 0, 1)
      # Check if we accept it
      if(u <= alpha){
        x[i]=sample.x
        finish=1
      }
    }
  }
  return (x)
}

#Testing function
alpha=0.5
sample.gamma1 <- random.gamma1(10000,alpha)
test.function(10000,random.gamma1,alpha,alpha,alpha)
```

The printout shows that the empirical mean and variance are close to the analitical values so the implementation seems correct.

Now we consider the gamma distribution with parameters $\alpha>1$ and $\beta=1$. To use ratio of uniforms method to simulate from this distrubution, we need to find $C_f\subset[0,a] \times [b_-,b_+]$. 
First we find the value of $a=\sqrt{sup_xf^*(x)}$. 
$$
\frac{d}{dx}f^*(x)=(\alpha-1)x^{\alpha-2}e^{-x}-e^{-x}x^{\alpha-1}=e^{-x}x^{\alpha-2}(\alpha-1-x)=0 \Rightarrow x=\alpha-1 	\lor x=0
$$
$f^*(x)$ has its maximum when $x=\alpha-1$, hence $a=\sqrt{f^*(\alpha-1)}=\sqrt{(\alpha-1)^{\alpha-1}e^{-\alpha+1}}$.

Then, we have to find the values of $b_+=\sqrt{sup_{x\geq0}x^2f^*(x)}$ and $b_-=-\sqrt{sup_{x\leq0}x^2f^*(x)}$. Since $f^*(x)=0$ for $x\leq0$, we have $b_-=0$.
$$
\frac{d}{dx}x^2f^*(x)=e^{-x}x^{\alpha}(\alpha+1-x)=0 \Rightarrow x=\alpha+1 	\lor x=0
$$
$x^2f^*(x)$ has its maximum when $x=\alpha+1$, hence $b_+=\sqrt{(\alpha+1)f^*(\alpha+1)}=\sqrt{(\alpha+1)^{\alpha+1}e^{-\alpha-1}}$.

To avoid overflow, we implement the algorithm on log-scale, $u=log(x)$. Hence, we define $C_f=\{ (u_1,u_2):0\leq u_1 \leq 0.5{f^*(e^{u_2-u_1})}\}$ where
$$
f^*(e^u)=\begin{cases} u(\alpha-1)-e^u, & 0<x \\ 0, & othetwise \end{cases}.
$$

```{r, include=TRUE}
logf <- function(u,alpha){ #log of f* with u=log(x)
  return (u*(alpha-1)-exp(u))
}

sample <- function(ab){ #This function sample from the distribution of log(x), using the invers of the cumulative distribution. 
  u=log(runif(1,0,1))
  return (ab+u)
}
```

```{r}
#This function returns a vector of n independent samples from the gamma distribution with a>1, beta=1, generated by the ratio of uniforms method on log-scale. 
random.gamma2 <- function(n,alpha){
  count=0 #count how many tries the algorithm needs to generate n realisations
  x=rep(0,n)
  a=(alpha-1)/2*(log(alpha-1)-1)
  b=(alpha+1)/2*(log(alpha+1)-1)
  for (i in 1:n){
    inside=FALSE
    while (inside==FALSE){
      u1=sample(a) 
      u2=sample(b)
      u=u2-u1
      test=0.5*logf(u,alpha)
      inside=test>=u1 
      count=count+1
    }
    x[i] = u
  }
  vec=c(count,exp(x)) #count + n realisations
}

#Testing function 
alpha=2000
ex.gamma2=random.gamma2(1000,alpha)
cat("Empirical mean:",mean(ex.gamma2[-1]), "Analytical mean:",alpha, "\n")
cat("Empirical variance:", var(ex.gamma2[-1]), "Analytical variance:",alpha,"\n")
```

The empirical mean and variance are close to the analytical values so the implementation seems correct.

```{r}
#Generate a plot with values of alpha on the x-axis and the number of tries used on the y-axis. 
plot.count <- function(nsample, n){
  count=rep(0,n)
  alp=seq(1.01,2000,length.out=n)
  for (i in 1:n){
    alpha=alp[i]
    c=random.gamma2(nsample,alpha)[1]
    count[i]=c
  }
  plot(alp,count)
}
plot.count(1000,100)
```

From the plot we can see that the number of tries the algorithm needs to generate $n=1000$ realisations increases almost linearly with the value of $\alpha$. SJEKK DETTE ETTER AT VI FÅR PLOTTET FOR ALLE ALPHA

By using the above functions and $X \sim Ga(\alpha,1) \Leftrightarrow X/\beta \sim Ga(\alpha, \beta)$, we write a function that generates a vector of n independent samples from the gamma distribution with parameters $\alpha$ and $\beta$. 
```{r}
##This function returns a vector of n independent samples from the gamma distribution with parameters alpha and beta. 
random.gamma <- function(n, alpha, beta){
  if (alpha==1){
    x=random.exp(n,alpha) #sample from exponential distribution
  }
  else if (alpha<1){
    x=random.gamma1(n,alpha) 
  }
  else {
    x=random.gamma2(n,alpha)[-1] #ignoring the first element, which is the number of tries. 
  }
  return (x/beta) #Transform from x-Ga(alpha,1) to x/beta-Ga(alpha,beta)
}

#Testing function 
test.gamma <- function(n,alpha, beta){
  ex.gamma=random.gamma(n,alpha,beta)
  amean=alpha/beta #analytical mean
  avar=alpha/beta^2 #analytical variance
  cat("Empirical mean:", mean(ex.gamma), "Analytical mean:",amean, "\n")
  cat("Empirical variance:", var(ex.gamma), "Analytical variance:",avar,"\n")
}

plot.gamma <- function(n,alpha,beta){
  sample.gamma <- enframe(random.gamma(n,alpha=alpha,beta=beta))
  ggplot() +
    geom_histogram(
      data = data.frame(sample.gamma),
      mapping = aes(x=value, y=..density..),
      binwidth = 0.01,
      boundary = 0,
      na.rm = TRUE
    ) +
    geom_vline(
      aes(xintercept=mean(sample.gamma$value),
          col='Empirical mean')
    ) +
    stat_function(
      fun = partial(dgamma, shape=alpha, rate=beta),
      aes(col='Theoretical density')
    ) +
    ylim(0,3) +
    xlim(0,5)
}
test.gamma(100000,alpha=1,beta=3)
test.gamma(100000,alpha=1,beta=3)
test.gamma(100000,alpha=4,beta=10)
plot.gamma(100000,0.5,0.7)
plot.gamma(100000,1,3)
plot.gamma(100000,4,10)

```

# Problem C: The Dirichlet distribution: simulation using known relations

Let $x=(x_1,\dots,x_K)$ be a vector of stochastic random variables where $x_k\in[0,1]$ for $k=1,\dots,K$ and $\sum_{k=1}^Kx_k=1$. 
We assume $z_k\sim \mathrm{gamma}(\alpha_k,1)$ independently for $k=1,\dots,K$. The joint distribution is then given by 

$$
f_Z(z_1,\dots,z_K) =
\prod_{k=1}^K f_Z(z_k) = 
\frac{1}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K}z_k^{\alpha_k-1} \dot{} e^{-\sum_{k=1}^Kz_k}.
$$

Let $v=\sum_{i=1}^Kz_i$ and $x_k=\frac{z_k}{z_1,\dots,z_K} = \frac{z_k}{v}$. Then $z_k = g^{-1}(x_k,v) = vx_k$ and because the $x_i$ sum to 1, the K-th variable can be written as $z_K=(1-\sum_{k=1}^{K-1}x_i)v$. We can now find the Jacobian 

\begin{align*}
\left\lvert \frac{\partial (z_1, \dots,z_K)}{\partial (x_1,\dots,x_{K-1},v)} \right\rvert 
&=\left\lvert \frac{\partial (vx_1, \dots,vx_{K-1}, v(1-\sum_{k=1}^{K-1}x_k))}{\partial (x_1,\dots,x_{K-1},v)} \right\rvert \\

&= 
\begin{vmatrix}
v & 0 & \cdots & 0 & x_1 \\ 
0 & \ddots & \ddots & \vdots & \vdots \\
\vdots & \ddots & \ddots & 0 &\vdots \\
0 & \cdots & 0 & v & x_{K-1} \\
-v & \cdots & \cdots & -v & 1-\sum_{k=1}^{K-1}x_k
\end{vmatrix}
= 
\begin{vmatrix}
v & 0 & \cdots & 0 & x_1 \\ 
0 & \ddots & \ddots & \vdots & \vdots \\
\vdots & \ddots & \ddots & 0 &\vdots \\
0 & \cdots & 0 & v & x_{K-1} \\
0 & \cdots & \cdots & 0 & 1 
\end{vmatrix}
=v^{K-1}.
\end{align*}

Thus, by the transformation formula we get the joint distribution

\begin{align}
f_{X,V}(x_1,\dots,x_{K-1},v)
&= f_Z(vx_1,\dots,vx_{K-1},v\left(1-\sum_{k=1}^{K-1}x_k)\right) \dot{} \left\lvert \frac{\partial \left(vx_1, \dots,vx_{K-1}, v\left(1-\prod_{k=1}^{K-1}x_k\right)\right)}{\partial (x_1,\dots,x_{K-1},v)} \right\rvert \\
&= \frac{1}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}(vx_k)^{\alpha_k-1} \dot{} \left(v\left(1-\prod_{k=1}^{K-1}x_k\right)\right)^{\alpha_K-1} \dot{} e^{-\sum_{k=1}^Kvx_k} \dot{} v^{K-1} \\
&= \frac{v^{\sum_{k=1}^{K-1}(\alpha_k-1)+(\alpha_K-1)+(K-1)}e^{-v}}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} \left(1-\prod_{k=1}^{K-1}x_k\right)^{\alpha_K-1} \\
&= \frac{v^{(\sum_{k=1}^{K}\alpha_k)-1}e^{-v}}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} \left(1-\prod_{k=1}^{K-1}x_k\right)^{\alpha_K-1}
\end{align}

And by integration the marginal distribution of $(x_1,\dots,x_{K-1})$ is
$$
f_X(x_1,\dots,x_{K-1})=  \frac{\int_0^\infty v^{(\sum_{k=1}^{K}\alpha_k)-1}e^{-v}dv}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} \left(1-\prod_{k=1}^{K-1}x_k\right)^{\alpha_K-1}
$$
$$= \frac{\Gamma(\sum_{k=1}^{K}\alpha_i)}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} \left(1-\prod_{k=1}^{K-1}x_k\right)^{\alpha_K-1}
$$
which means $(x_1,\dots,x_K)$ has a Dirichlet distribution.

```{r}
##This function generates one realisation from Dirichlet distribution with parameter vector alpha.
random.dirichlet <- function(alpha){
  K <- length(alpha)
  z <- rep(0,K)
  for (i in 1:K)
    z[i] <- random.gamma(1,alpha[i],1) #n,shape,rate
  v <- sum(z)
  return (z/v)
}

N_dir <- 1000
alpha <- c(.1, .5, .1, .3, .8)
K <- length(alpha)
dirichlet.sample <- matrix(1,N_dir,K)
for (i in 1:N_dir){
  dirichlet.sample[i,] <- random.dirichlet(alpha)
}
```
The mean and variance of a random variable $X_i$ from a Dirichlet distribution are
$$
\text{E}(X_i) = \frac{\alpha_i}{\sum_{k=1}^K\alpha_k} := \tilde \alpha_i
$$ and
$$
\text{Var}(X_i)=\frac{\tilde{\alpha_i}(1-\tilde \alpha_i)}{\sum_{k=1}^K\alpha_k +1}.
$$
```{r, inclue=TRUE}
alphasum <- sum(alpha)
#Empirical mean
colMeans(dirichlet.sample)
#Analytical mean
alpha1 <- alpha/alphasum
alpha1
#Empirical variance
diag(var(dirichlet.sample))
#Analytical variance
alpha1*(1-alpha1)/(1+alphasum)
```
The empirical mean and variance are very close to the analytical values.

# Problem D: Rejection sampling and importance sampling
In this exercise we are interested in the posterior mean $E\big[\theta|y\big]$. For $\theta \in (0,1)$ the observed posterior density is $f(\theta|y) \propto (2+\theta)^{125}(1-\theta)^{38}\theta^{34}$.

First we implement a rejection sampling algorithm to simulate from $f(\theta|y)$ using the uniform distribution $\mathcal{U}(0, 1)$ as the proposal density. Hence, the acceptance probability is $\alpha=\frac{1}{c}f(x)$

```{r}
f <- function(theta){ #The proportional function
  return ((2+theta)^125*(1-theta)^(18+20)*theta^34) 
}

#Find the maxima of the proportional function 
res = optimise(f, interval = c(0,1),maximum=TRUE) 
maxima=res$objective
  
logf <- function(theta){
  return (log(f(theta)))
}

cf <- function(theta){
  return (f(theta) / maxima)
}

theta_new_f <- function(theta){ #theta*new_f
  return (theta*new_f(theta))
}

new_f <- function(theta){
  return (f(theta)/as.numeric(integrate(f,0,1)[1])) 
}
```
```{r}
#This algorithm simulate from f(theta|y) using a uniform density as the proposal density, rejection sampling.
random.multinomial <- function(n){
  x.out <- rep(NA,n)
  rejections <- 0
  for (i in 1:n){
    finished <- FALSE
    c <- maxima #res$objective  #f(15/394+sqrt(53809)/394)
    while(!finished){
      x <- runif(1) #sample from the proposal distribution, U(0,1)
      alpha <- f(x) / c #compute the acceptance probability alpha
      u <- runif(1) #generate a helping variable U from a uniform(0,1)
      if (u <= alpha){ #Check if we accept it 
        finished <- TRUE
      }
      else{
        rejections = rejections + 1 #count the rejections
      }
    }
    x.out[i] <- x #appending the values that get accepted (u<=alpha)
  }
  cat(rejections,"rejections out of",(n+rejections),"trials.(",(rejections)/(n+rejections)*100, "% )\n")
  return (x.out)
}
```
To estimate the posterior mean of $\theta$ we use Monte-Carlo integration with M=10000 samples from $f(\theta|y)$. 

```{r}
test <- random.multinomial(10000)
cat("Estimated posterior mean:", mean(test), "\n")
hist(test, prob=TRUE)
xlin <- seq(0,1,.01)
lines(new_f(xlin))
curve(new_f,0,1)
```

To check the value of the estimated posterior mean, we approximate it
by numerically solving the integral 
$$
E\big[\theta|y\big] = \int_0^1 \theta \cdot f(\theta | \textbf{y})~\text{d}\theta.
$$
```{r}
expected_mean=integrate(theta_new_f,0,1) 
cat("Expected posterior mean:", expected_mean$value, "\n")
```

To estimate the posterior mean under the new prior $g(\theta) \sim Beta(1,5)$ we use importance sampling with samples from $f(\theta|y)$. Since neither $f(\theta) \propto 1$ and $g(\theta) \propto (1-\theta)^4$ are normalize, we use self-normalizing importance sampling to calculate the new posterior mean.

$$
\tilde{\mu}=\frac{\sum_{i=1}^{n}h(\theta_i)w(\theta_i)}{\sum_{i=1}^{n}w(\theta_i)},
$$
where $h(\theta_i)=\theta_i$ is the samples from $f(\theta|y)$ and 
$$
w(\theta_i)=\frac{g(\theta_i)}{f(\theta_i)} \propto (1-\theta)^4.
$$

```{r}
importance_sampling <- function(n){ 
  theta <- random.multinomial(n) #samples from f(theta|y) with the rejection sampling
  w=(1-theta)^4 #importance weight
  mu <- sum(theta*w)/sum(w) #self-normalizing 
  return(mu)
}

mu.estimate <- importance_sampling(10000)

sample.multinomial <- enframe(random.multinomial(100000))

ggplot() +
    geom_histogram(
      data = data.frame(sample.multinomial),
      mapping = aes(x=value, y=..density..),
      binwidth = 0.001,
      boundary = 0,
      na.rm = TRUE
    ) +
    geom_vline(
      aes(xintercept=mean(sample.multinomial$value),
          col='Empirical mean')
    )
```
