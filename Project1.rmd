---
title: "Computer Intensive Statistical Methods-Exercise 1"
author: "Vebjørn Rekkebo, Camilla Karlsen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

##1)

```{r, include=TRUE}
#This function returns a vector of n independent samples from the exponential distribution with rate parameter lam, generated from the inverse cumulative distribution. 
random.exp <- function(n,lam){
  return(-1/lam * log(runif(n)))
}

#Testing function
lam <- 3
exp.sample <- random.exp(10000,lam)
#Empirical mean and variance
mean(exp.sample)
var(exp.sample)
#Analytical mean and variance
1/lam
1/lam^2
```

```{r, include=TRUE}
#This function returns a vector of n independent samples from the g distribution with parameter a, generated from the inverse cumulative distribution. 
random.g <- function(n,a){
  c <- a*exp(1)/(a+exp(1))
  u <- runif(n)
  x <- rep(0,n)
  for (i in 1:n){
    if (u[i]<c/a){
      x[i] <- (a*u[i]/c)^(1/a)
    }
    else{
      x[i] <- (-log(1/a + exp(-1) - u[i]/c))
    }
  }
  return(x)
}

#Testing funtion
a <- .6
g.sample <- random.g(10000, a)
#empirical mean and variance
g.sample.mean <- mean(g.sample)
g.sample.var <- var(g.sample)
#analytical mean and variance
c <- a*exp(1)/(a+exp(1))
c/(a+1)+2*c*exp(-1)
c*(1/(a+2)+ 5*exp(-1)) - g.sample.mean^2
```

```{r, include=TRUE}
#This function returns a vector of n independent samples from the standard normal distribution, generated from by the Box-Müller algorithm
random.norm <- function(n){
  x1 <- runif(n)*2*pi
  x2 <- random.exp(n,.5)
  return(sqrt(x2)*cos(x1))
}

#Testing function
normal.sample <- random.norm(10000)
mean(normal.sample)
var(normal.sample)
```

```{r}
#This function returns a sample from the d-variate normal distribution with mean mu and covariance S, transformed from standard normal samples. 
random.multinorm <- function(d,mu,S){
  x <- random.norm(d)
  A <- t(chol(S)) #want lower triangular
  return(mu + A%*%x)
}

#Testing function
d = 2
N = 10000
sigma <- cbind(c(2,1), c(1,2))
mu <- c(2,1)
multinormal.sample <- matrix(NA,N,d)
for (i in 1:N){
  multinormal.sample[i,] <- random.multinorm(d, mu, sigma)
}
colMeans(multinormal.sample)
var(multinormal.sample) #NOE GALT HER
```

##2)

##3)

##4)

#Problem B: The gamma distribution

##1) Rejection sampling
To use rejection sampling to generate samples from $f(x)$, we need a $c>1$ such that $c\geq\frac{f(x)}{g(x)}$.
$$
\frac{f(x)}{g(x)}=\begin{cases}\frac{1}{k\Gamma(\alpha)}e^{-x}, & 0<x<1 \\ \frac{1}{k\Gamma(\alpha)}x^{\alpha -1}, & 1 \leq x \end{cases}\leq \frac{1}{k\Gamma(\alpha)}=c
$$
The acceptance probability in the rejection sampling is

$$
\alpha=\frac{1}{c}\frac{f(x)}{g(x)}=\begin{cases}e^{-x}, & 0<x<1 \\ x^{\alpha -1}, & 1 \leq x \end{cases}.
$$

```{r, include=TRUE}
k <- function(a){
  return (a*exp(1)/(a+exp(1)))
}

f <- function(x, a){ 
  return (1/gamma(a)*x^(a-1)*exp(-x))
}

#Acceptance probability
accept_prob <- function(x,a){ 
  if (x<1)
      return (exp(-x))
    else 
      return (x^(a-1))
}
```

```{r, include=TRUE}
#This function returns a vector of n independent samples from the gamma distribution, generated by the Rejection sampling
f1 <- function(n,a){ 
  x=rep(0,n)
  for (i in 1:n){
    finish = 0
    while(finish==0){  
      ## Sample from the proposal distribution, g(x) (x>=0)
      sample.x = random.g(1,a)
      ## Compute the acceptance probability alpha
      alpha=accept_prob(sample.x,a)
      ## Generate a helping variable U from a uniform(0,1)
      u = runif(1, 0, 1)
      ## Check if we accept it
      if(u <= alpha){
        x[i]=sample.x
        finish=1
      }
    }
  }
  return (x)
}

#Testing function
test=f1(10000,0.5)
var=var(test) #=alpha
mean=mean(test) #=alpha
hist(test)
mean
var

```
##2) Ratio of uniforms method 
```{r, include=TRUE}
logf <- function(u,alpha){
  return (u*(alpha-1)-exp(u))
}

a <- function(alpha){
  return (sqrt((alpha-1)^(alpha-1)*exp(-alpha+1)))
}

b <- function(alpha){
  return (sqrt((alpha+1)^(alpha+1)*exp(-alpha-1)))
}

sample <- function(ab){
  u=runif(1,0,1)
  return (log(ab*u))
}

f2 <- function(n,alpha){
  count=0
  x=rep(0,n)
  a=sqrt((alpha-1)^(alpha-1)*exp(-alpha+1))
  b=sqrt((alpha+1)^(alpha+1)*exp(-alpha-1))
  for (i in 1:n){
    inside=FALSE
    while (inside==FALSE){
      y1=sample(a)
      y2=sample(b)
      y=y2-y1
      inside=0.5*logf(y,alpha)>=y1
      count=count+1
    }
    x[i] = y 
  }
  ret=c(count,exp(x))
  return (ret)
}

#Testing function 
n=1000
test=f2(n,10)
count=test[1]
var=var(test[-1]) #=alpha
mean=mean(test[-1]) #=alpha
hist(test[-1])
count
mean
var

plot.count <- function(nsample, n){
  count=rep(0,n)
  alp=seq(1,140,length.out=n)
  for (i in 1:n){
    alpha=alp[i]
    c=f2(nsample,alpha)[1]
    count[i]=c
  }
  plot(alp,count)
}
plot.count(1000,10)
```
##3) Gamma distribution with parameters α and β
$X \sim Ga(\alpha,1) \Leftrightarrow X/\beta \sim Ga(\alpha, \beta)$
```{r,include=TRUE}
gamma <- function(n, alpha, beta){
  if (alpha==1){
    x=random.exp(n,alpha)
  }
  else if (alpha>1){
    x=f2(n,alpha)[-1]
  }
  else {
    x=f1(n,alpha)
  }
  return (x/beta)
}

#Testing function 
test.gamma <- function(n,alpha, beta){
  test=gamma(n,alpha,beta)
  var=var(test)
  mean=mean(test)
  hist(test)
  print(mean)
  print(alpha/beta)
  print(var)
  print(alpha/beta^2)
}

test.gamma(10000,0.5,0.7)
test.gamma(10000,1,3)
test.gamma(10000,4,10)
```

#Problem C: The Dirichlet distribution: simulation using known relations

##1)
Let $x=(x_1,\dots,x_K)$ be a vector of stochastic random variables where $x_k\in[0,1]$ for $k=1,\dots,K$ and $\sum_{k=1}^Kx_k=1$. 
We assume $z_k\sim$gamma$(\alpha_k,1)$ independently for $k=1,\dots,K$. The joint distribution is then given by 
$$
f_Z(z_1,\dots,z_K) =
\prod_{k=1}^K f_Z(z_k) = 
\frac{1}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K}z_k^{\alpha_k-1} \dot{} e^{-\sum_{k=1}^Kz_k}.
$$
Let $v=\sum_{i=1}^K$ and $x_k=\frac{z_k}{z_1,\dots,z_K} = \frac{z_k}{v}$. Then $z_k = g^{-1}(x_k,v) = vx_k$ and because the $x_i$ sum to 1, the K-th variable can be written as $z_K=1-\sum_{k=1}^{K-1}x_i$. We can now find the Jacobian 
\begin{align*}
\left\lvert \frac{\partial z_1, \dots,z_K}{\partial x_1,\dots,x_{K-1},v} \right\rvert 
&=\left\lvert \frac{\partial vx_1, \dots,vx_{K-1}, v(1-\sum_{k=1}^{K-1}x_k)}{\partial x_1,\dots,x_{K-1},v} \right\rvert \\

&= 
\begin{vmatrix}
v & 0 & \cdots & 0 & x_1 \\ 
0 & \ddots & \ddots & \vdots & \vdots \\
\vdots & \ddots & \ddots & 0 &\vdots \\
0 & \cdots & 0 & v & x_{K-1} \\
-v & \cdots & \cdots & -v & 1-\sum_{k=1}^{K-1}x_k
\end{vmatrix}
= 
\begin{vmatrix}
v & 0 & \cdots & 0 & x_1 \\ 
0 & \ddots & \ddots & \vdots & \vdots \\
\vdots & \ddots & \ddots & 0 &\vdots \\
0 & \cdots & 0 & v & x_{K-1} \\
0 & \cdots & \cdots & 0 & 1 
\end{vmatrix}
=v^{K-1}.
\end{align*}
Thus, by the transformation formula we get the joint distribution
\begin{align}
f_{X,V}(x_1,\dots,x_{K-1},v)
&= f_Z(vx_1,\dots,vx_{K-1},v(1-\sum_{k=1}^{K-1}x_k)) \dot{} \left\lvert \frac{\partial vx_1, \dots,vx_{K-1}, v(1-\sum_{k=1}^{K-1}x_k)}{\partial x_1,\dots,x_{K-1},v} \right\rvert \\
&= \frac{1}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}(vx_k)^{\alpha_k-1} \dot{} (v(1-\prod_{k=1}^{K-1}vx_k))^{\alpha_K-1} \dot{} e^{-\sum_{k=1}^Kvx_k} \dot{} v^{K-1} \\
&= \frac{v^{\sum_{k=1}^{K-1}(\alpha_i-1)+(\alpha_K-1)+(K-1)}e^{-v}}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1} \\
&= \frac{v^{(\sum_{k=1}^{K}\alpha_i)-1}e^{-v}}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1}
\end{align}
And by integration the marginal distribution of $(x_1,\dots,x_{K-1})$ is
$$
f_X(x_1,\dots,x_{K-1})=  \frac{\int_0^\infty v^{(\sum_{k=1}^{K}\alpha_i)-1}e^{-v}dv}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1}
$$
$$
=f_X(x_1,\dots,x_{K-1})=  \frac{\Gamma(\sum_{k=1}^{K}\alpha_i)}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1}
$$
which is the Dirichlet distribution.

##2)

```{r}

#### FEIL GAMMAFUNKSJON
random.dirichlet <- function(alpha){
  K <- length(alpha)
  z <- rgamma(K,alpha,1) #n,shape,rate
  v <- sum(z)
  return (z/v)
}

N_dir <- 1000
alpha <- c(.1, .5, .1, .3, .8)
K <- length(alpha)
dirichlet.sample <- matrix(NA,N_dir,K)
for (i in 1:N_dir){
  dirichlet.sample[i,] <- random.dirichlet(alpha)
}
#Empirical mean and variance
colMeans(dirichlet.sample)
diag(var(dirichlet.sample))

alphasum <- sum(alpha)
#Analytical mean and variance
alpha1 <- alpha/alphasum
alpha1
alpha1*(1-alpha1)/(1+alphasum)
```

#Problem D: Rejection sampling and importance sampling

##1)
```{r}
random.multinomial <- function(n){
  x <- runif()
  u <- runif()
  
}
```

##2)

##3)

##4)