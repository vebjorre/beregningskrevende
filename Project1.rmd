---
title: "Computer Intensive Statistical Methods-Exercise 1"
author: "Vebjørn Rekkebo, Camilla Karlsen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

```{r, include=TRUE}
#This function returns a vector of n independent samples from the exponential distribution with rate parameter lam, generated from the inverse cumulative distribution. 
random.exp <- function(n,lam){
  return(-1/lam * log(runif(n)))
}

#Testing function
test.function <- function(n,func,arg,amean,avar){
  exs.sample=func(n,arg)
  cat("Empirical mean:", mean(exs.sample), "Analytical mean:",amean, "\n")
  cat("Empirical variance:", var(exs.sample), "Analytical variance:",avar,"\n")
  hist(exs.sample)
}

#Bruk funksjon eller ikke? 

lam <- 3
ex.exp <- random.exp(10000,lam)
cat("Empirical mean:", mean(ex.exp), "Analytical mean:",1/lam, "\n")
cat("Empirical variance:", var(ex.exp), "Analytical variance:",1/lam^2,"\n")
hist(ex.exp)

#test.function(10000,random.exp,lam,1/lam,1/lam^2)

```
HER MÅ CDF OG INVERSE LEGGES INN (2a)
```{r, include=TRUE}
#This function returns a vector of n independent samples from the g distribution with parameter a, generated from the inverse cumulative distribution. 

random.g <- function(n,a){
  c <- (a*exp(1)/(a+exp(1)))
  u <- runif(n)
  x <- rep(0,n)
  for (i in 1:n){
    if (u[i]<c/a){
      x[i] <- (a*u[i]/c)^(1/a)
    }
    else{
      x[i] <- (-log(1/a + exp(-1) - u[i]/c))
    }
  }
  return(x)
}

c <- function(a){
  return (a*exp(1)/(a+exp(1)))
}

#Testing funtion
a <- .6
c <- c(a)
amean <- c/(a+1)+2*c*exp(-1) #analytical mean
avar <- c*(1/(a+2)+ 5*exp(-1)) - amean^2 #analytical variance
test.function(10000,random.g,a,amean,avar)
```

```{r, include=TRUE}
#This function returns a vector of n independent samples from the standard normal distribution, generated from by the Box-Müller algorithm
random.norm <- function(n){
  x1 <- runif(n)*2*pi
  x2 <- random.exp(n,.5)
  return(sqrt(x2)*cos(x1))
}

#Testing function
ex.normal=random.norm(10000)
cat("Empirical mean:", mean(ex.normal), "Analytical mean:",0.0, "\n")
cat("Empirical variance:", var(ex.normal), "Analytical variance:",1.0,"\n")
hist(ex.normal)
```

```{r}
#This function returns a sample from the d-variate normal distribution with mean mu and covariance S, transformed from standard normal samples. 
random.multinorm <- function(d,mu,S){
  x <- random.norm(d)
  A <- t(chol(S)) #want lower triangular
  return(mu + A%*%x)
}

#Testing function
d = 2
N = 10000
sigma <- cbind(c(2,1), c(1,2))
mu <- c(2,1)
multinormal.sample <- matrix(NA,N,d)
for (i in 1:N){
  multinormal.sample[i,] <- random.multinorm(d, mu, sigma)
}
colMeans(multinormal.sample)
var(multinormal.sample)
```


#Problem B: The gamma distribution

##Rejection sampling
To use rejection sampling to generate samples from $f(x)$, we need a $k>1$ such that $k\geq\frac{f(x)}{g(x)}$.
$$
\frac{f(x)}{g(x)}=\begin{cases}\frac{1}{c\Gamma(\alpha)}e^{-x}, & 0<x<1 \\ \frac{1}{c\Gamma(\alpha)}x^{\alpha -1}, & 1 \leq x \end{cases}\leq \frac{1}{c\Gamma(\alpha)}=k
$$
The acceptance probability in the rejection sampling is

$$
\alpha=\frac{1}{k}\frac{f(x)}{g(x)}=c\Gamma(\alpha)\frac{f(x)}{g(x)}=\begin{cases}e^{-x}, & 0<x<1 \\ x^{\alpha -1}, & 1 \leq x \end{cases}.
$$

```{r, include=TRUE}
#Acceptance probability
accept_prob <- function(x,a){ 
  if (x<1)
      return (exp(-x))
    else 
      return (x^(a-1))
}
```

```{r, include=TRUE}
#This function returns a vector of n independent samples from the gamma distribution with 0<a<1, beta=1, generated by the Rejection sampling
random.gamma1 <- function(n,a){ 
  x=rep(0,n)
  for (i in 1:n){
    finish = 0
    while(finish==0){  
      # Sample from the proposal distribution, g(x) (x>=0)
      sample.x = random.g(1,a)
      # Compute the acceptance probability alpha
      alpha=accept_prob(sample.x,a)
      # Generate a helping variable U from a uniform(0,1)
      u = runif(1, 0, 1)
      # Check if we accept it
      if(u <= alpha){
        x[i]=sample.x
        finish=1
      }
    }
  }
  return (x)
}

#Testing function
alpha=0.5
test.function(10000,random.gamma1,alpha,alpha,alpha)
```

##Ratio of uniforms method 
First we find the value of $a=\sqrt{sup_xf^*(x)}$. 
$$
\frac{d}{dx}f^*(x)=(\alpha-1)x^{\alpha-2}e^{-x}-e^{-x}x^{\alpha-1}=e^{-x}x^{\alpha-2}(\alpha-1-x)=0 \Rightarrow x=\alpha-1 	\lor x=0
$$
$f^*(x)$ has its maximum when $x=\alpha-1$, hence $a=\sqrt{f^*(\alpha-1)}=\sqrt{(\alpha-1)^{\alpha-1}e^{-\alpha+1}}$.

Then, we have to find the values of $b_+=\sqrt{sup_{x\geq0}x^2f^*(x)}$ and $b_-=-\sqrt{sup_{x\leq0}x^2f^*(x)}$. Since $f^*(x)=0$ for $x\leq0$, we have $b_-=0$.
$$
\frac{d}{dx}x^2f^*(x)=e^{-x}x^{\alpha}(\alpha+1-x)=0 \Rightarrow x=\alpha+1 	\lor x=0
$$
$x^2f^*(x)$ has its maximum when $x=\alpha+1$, hence $b_+=\sqrt{(\alpha+1)f^*(\alpha+1)}=\sqrt{(\alpha+1)^{\alpha+1}e^{-\alpha-1}}$.

$C_f\subset[0,a] \times [0,b_+]$


```{r, include=TRUE}
a <- function(alpha){ 
  return (sqrt((alpha-1)^(alpha-1)*exp(-alpha+1)))
}

b <- function(alpha){ #b+
  return (sqrt((alpha+1)^(alpha+1)*exp(-alpha-1)))
}

logf <- function(u,alpha){
  return (u*(alpha-1)-exp(u))
}

sample <- function(ab){
  u=runif(1,0,1)
  return (log(ab*u))
}
```

```{r}
#This function returns a vector of n independent samples from the gamma distribution with a>1, beta=1, generated by the ratio of uniforms method on log-scale. 
random.gamma2 <- function(n,alpha){
  count=0 #count how many tries the algorithm needs to generate n realisations
  x=rep(0,n)
  a=sqrt((alpha-1)^(alpha-1)*exp(-alpha+1))
  b=sqrt((alpha+1)^(alpha+1)*exp(-alpha-1))
  for (i in 1:n){
    inside=FALSE
    while (inside==FALSE){
      y1=sample(a) 
      y2=sample(b)
      y=y2-y1
      inside=0.5*logf(y,alpha)>=y1 
      count=count+1
    }
    x[i] = y 
  }
  vec=c(count,exp(x)) #count + n realisations
}

#Testing function 
alpha=10 #FUNKER IKKE FOR ALPHA>140......
ex.gamma2=random.gamma2(1000,alpha)
cat("Empirical mean:",mean(ex.gamma2[-1]), "Analytical mean:",alpha, "\n")
cat("Empirical variance:", var(ex.gamma2[-1]), "Analytical variance:",alpha,"\n")
hist(ex.gamma2[-1])

#Generate a plot with values of alpha on the x-axis and the number of tries used on the y-axis. 
plot.count <- function(nsample, n){
  count=rep(0,n)
  alp=seq(1,140,length.out=n)
  for (i in 1:n){
    alpha=alp[i]
    c=random.gamma2(nsample,alpha)[1]
    count[i]=c
  }
  plot(alp,count)
}
plot.count(1000,10)
```

##Gamma distribution with parameters α and β
$X \sim Ga(\alpha,1) \Leftrightarrow X/\beta \sim Ga(\alpha, \beta)$
```{r}
##This function returns a vector of n independent samples from the gamma distribution with parameters alpha and beta. 
random.gamma <- function(n, alpha, beta){
  if (alpha==1){
    x=random.exp(n,alpha)
  }
  else if (alpha<1){
    x=random.gamma1(n,alpha)
  }
  else {
    x=random.gamma2(n,alpha)[-1]
  }
  return (x/beta) #Transform from x-Ga(alpha,1) to x/beta-Ga(alpha,beta)
}

#Testing function 
test.gamma <- function(n,alpha, beta){
  ex.gamma=random.gamma(n,alpha,beta)
  amean=alpha/beta #analytical mean
  avar=alpha/beta^2 #analytical variance
  cat("Empirical mean:", mean(ex.gamma), "Analytical mean:",amean, "\n")
  cat("Empirical variance:", var(ex.gamma), "Analytical variance:",avar,"\n")
  hist(ex.gamma)
}

test.gamma(10000,alpha=0.5,beta=0.7)
test.gamma(10000,alpha=1,beta=3) 
test.gamma(10000,alpha=4,beta=10)
```

#Problem C: The Dirichlet distribution: simulation using known relations
Let $x=(x_1,\dots,x_K)$ be a vector of stochastic random variables where $x_k\in[0,1]$ for $k=1,\dots,K$ and $\sum_{k=1}^Kx_k=1$. 
We assume $z_k\sim \mathrm{gamma}(\alpha_k,1)$ independently for $k=1,\dots,K$. The joint distribution is then given by 

$$
f_Z(z_1,\dots,z_K) =
\prod_{k=1}^K f_Z(z_k) = 
\frac{1}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K}z_k^{\alpha_k-1} \dot{} e^{-\sum_{k=1}^Kz_k}.
$$

Let $v=\sum_{i=1}^K$ and $x_k=\frac{z_k}{z_1,\dots,z_K} = \frac{z_k}{v}$. Then $z_k = g^{-1}(x_k,v) = vx_k$ and because the $x_i$ sum to 1, the K-th variable can be written as $z_K=1-\sum_{k=1}^{K-1}x_i$. We can now find the Jacobian 

\begin{align*}
\left\lvert \frac{\partial z_1, \dots,z_K}{\partial x_1,\dots,x_{K-1},v} \right\rvert 
&=\left\lvert \frac{\partial vx_1, \dots,vx_{K-1}, v(1-\sum_{k=1}^{K-1}x_k)}{\partial x_1,\dots,x_{K-1},v} \right\rvert \\

&= 
\begin{vmatrix}
v & 0 & \cdots & 0 & x_1 \\ 
0 & \ddots & \ddots & \vdots & \vdots \\
\vdots & \ddots & \ddots & 0 &\vdots \\
0 & \cdots & 0 & v & x_{K-1} \\
-v & \cdots & \cdots & -v & 1-\sum_{k=1}^{K-1}x_k
\end{vmatrix}
= 
\begin{vmatrix}
v & 0 & \cdots & 0 & x_1 \\ 
0 & \ddots & \ddots & \vdots & \vdots \\
\vdots & \ddots & \ddots & 0 &\vdots \\
0 & \cdots & 0 & v & x_{K-1} \\
0 & \cdots & \cdots & 0 & 1 
\end{vmatrix}
=v^{K-1}.
\end{align*}

Thus, by the transformation formula we get the joint distribution

\begin{align}
f_{X,V}(x_1,\dots,x_{K-1},v)
&= f_Z(vx_1,\dots,vx_{K-1},v(1-\sum_{k=1}^{K-1}x_k)) \dot{} \left\lvert \frac{\partial vx_1, \dots,vx_{K-1}, v(1-\sum_{k=1}^{K-1}x_k)}{\partial x_1,\dots,x_{K-1},v} \right\rvert \\
&= \frac{1}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}(vx_k)^{\alpha_k-1} \dot{} (v(1-\prod_{k=1}^{K-1}vx_k))^{\alpha_K-1} \dot{} e^{-\sum_{k=1}^Kvx_k} \dot{} v^{K-1} \\
&= \frac{v^{\sum_{k=1}^{K-1}(\alpha_i-1)+(\alpha_K-1)+(K-1)}e^{-v}}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1} \\
&= \frac{v^{(\sum_{k=1}^{K}\alpha_i)-1}e^{-v}}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1}
\end{align}

And by integration the marginal distribution of $(x_1,\dots,x_{K-1})$ is
$$
f_X(x_1,\dots,x_{K-1})=  \frac{\int_0^\infty v^{(\sum_{k=1}^{K}\alpha_i)-1}e^{-v}dv}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1}
$$
$$
=f_X(x_1,\dots,x_{K-1})=  \frac{\Gamma(\sum_{k=1}^{K}\alpha_i)}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1}
$$
which is the Dirichlet distribution.

```{r}
##This function generates one realisation from Dirichlet distribution with parameter vector alpha.
random.dirichlet <- function(alpha){#Har endret litt her. Ble det riktig? 
  K <- length(alpha)
  z <- rep(0,K)
  for (i in 1:K)
    z[i] <- random.gamma(1,alpha[i],1) #n,shape,rate
  v <- sum(z)
  return (z/v)
}

N_dir <- 1000
alpha <- c(.1, .5, .1, .3, .8)
K <- length(alpha)
dirichlet.sample <- matrix(1,N_dir,K)
for (i in 1:N_dir){
  dirichlet.sample[i,] <- random.dirichlet(alpha)
}

alphasum <- sum(alpha)
#Empirical mean
colMeans(dirichlet.sample)
#Analytical mean
alpha1 <- alpha/alphasum
alpha1
#Empirical variance
diag(var(dirichlet.sample))
#Analytical variance
alpha1*(1-alpha1)/(1+alphasum)
```

#Problem D: Rejection sampling and importance sampling

##1)
```{r}
f <- function(theta){
  return ((2+theta)^125*(1-theta)^(18+20)*theta^34)
}

res = optimise(f, interval = c(0,1),maximum=TRUE) 
  
logf <- function(theta){
  return (log((2+theta)^125*(1-theta)^(18+20)*theta^34))
}

cf <- function(theta){
  return ((2+theta)^125*(1-theta)^(18+20)*theta^34) / f(15/394+sqrt(53809)/394)
}

theta_new_f <- function(theta){
  return (theta*new_f(theta))
}

new_f <- function(theta){
  return (((2+theta)^125*(1-theta)^(18+20)*theta^34)/as.numeric(integrate(f,0,1)[1]))
}
```
```{r}
#This algorithm simulate from f(theta|y) using a uniform density as the proposal density, rejection sampling.
random.multinomial <- function(n){
  x.out <- rep(NA,n)
  rejections <- 0
  for (i in 1:n){
    finished <- FALSE
    c <- res$objective #f(15/394+sqrt(53809)/394)
    while(!finished){
      x <- runif(1) #sample from the proposal distribution, U(0,1)
      alpha <- f(x) / c #compute the acceptance probability alpha
      u <- runif(1) #generate a helping variable U from a uniform(0,1)
      if (u <= alpha){
        finished <- TRUE
      }
      else{
        rejections = rejections + 1
      }
    }
    x.out[i] <- x
  }
  cat(rejections,"rejections out of",(n+rejections),"trials.(",(rejections)/(n+rejections)*100, "% )")
  return (x.out)
}

test <- random.multinomial(10000)
mean(test)
var(test)
hist(test, prob=TRUE)
xlin <- seq(0,1,.01)
lines(new_f(xlin))
curve(new_f,0,1)


newc <- integrate(f, 0, 1)
integrate(theta_new_f,0,1)


importance_sampling <- function(n){ #mulig vi ikke kan bruke self-normalizing? 
  x <- random.multinomial(n)
  mu <- sum(x*(1-x)^4/f(x))/sum((1-x)^4/f(x))
  print(mu)
  mu1 <- sum(x*(1-x)^4)/sum((1-x)^4) #med f(x)=1 - prior istedenfor posterior?
  print(mu1)
  print(mean(x))
  return(mu1)
}

test2 <- importance_sampling(10000)
test2
```
