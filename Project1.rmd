---
title: "Computer Intensive Statistical Methods-Exercise 1"
author: "Vebjørn Rekkebo, Camilla Karlsen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

##1)

```{r, include=TRUE}
#This function returns a vector of n independent samples from the exponential distribution with rate parameter lam, generated from the inverse cumulative distribution. 
random.exp <- function(n,lam){
  return(-1/lam * log(runif(n)))
}

#Testing function
lam <- 3
exp.sample <- random.exp(10000,lam)
#Empirical mean and variance
mean(exp.sample)
var(exp.sample)
#Analytical mean and variance
1/lam
1/lam^2
```

```{r, include=TRUE}
#This function returns a vector of n independent samples from the g distribution with parameter a, generated from the inverse cumulative distribution. 
random.g <- function(n,a){
  c <- a*exp(1)/(a+exp(1))
  u <- runif(n)
  x <- rep(0,n)
  for (i in 1:n){
    if (u[i]<c/a){
      x[i] <- (a*u[i]/c)^(1/a)
    }
    else{
      x[i] <- (-log(1/a + exp(-1) - u[i]/c))
    }
  }
  return(x)
}

#Testing funtion
a <- .6
g.sample <- random.g(10000, a)
#empirical mean and variance
g.sample.mean <- mean(g.sample)
g.sample.var <- var(g.sample)
#analytical mean and variance
c <- a*exp(1)/(a+exp(1))
c/(a+1)+2*c*exp(-1)
c*(1/(a+2)+ 5*exp(-1)) - g.sample.mean^2
```

```{r, include=TRUE}
#This function returns a vector of n independent samples from the standard normal distribution, generated from by the Box-Müller algorithm
random.norm <- function(n){
  x1 <- runif(n)*2*pi
  x2 <- random.exp(n,.5)
  return(sqrt(x2)*cos(x1))
}

#Testing function
normal.sample <- random.norm(10000)
mean(normal.sample)
var(normal.sample)
```

```{r}
#This function returns a sample from the d-variate normal distribution with mean mu and covariance S, transformed from standard normal samples. 
random.multinorm <- function(d,mu,S){
  x <- random.norm(d)
  A <- t(chol(S)) #want lower triangular
  return(mu + A%*%x)
}

#Testing function
d = 2
N = 10000
sigma <- cbind(c(2,1), c(1,2))
mu <- c(2,1)
multinormal.sample <- matrix(NA,N,d)
for (i in 1:N){
  multinormal.sample[i,] <- random.multinorm(d, mu, sigma)
}
colMeans(multinormal.sample)
var(multinormal.sample) #NOE GALT HER
```

##2)

##3)

##4)

#Problem B: The gamma distribution

##1)

##2)

##3)

#Problem C: The Dirichlet distribution: simulation using known relations

##1)
Let $x=(x_1,\dots,x_K)$ be a vector of stochastic random variables where $x_k\in[0,1]$ for $k=1,\dots,K$ and $\sum_{k=1}^Kx_k=1$. 
We assume $z_k\sim$gamma$(\alpha_k,1)$ independently for $k=1,\dots,K$. The joint distribution is then given by 
$$
f_Z(z_1,\dots,z_K) =
\prod_{k=1}^K f_Z(z_k) = 
\frac{1}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K}z_k^{\alpha_k-1} \dot{} e^{-\sum_{k=1}^Kz_k}.
$$
Let $v=\sum_{i=1}^K$ and $x_k=\frac{z_k}{z_1,\dots,z_K} = \frac{z_k}{v}$. Then $z_k = g^{-1}(x_k,v) = vx_k$ and because the $x_i$ sum to 1, the K-th variable can be written as $z_K=1-\sum_{k=1}^{K-1}x_i$. We can now find the Jacobian 
\begin{align*}
\left\lvert \frac{\partial z_1, \dots,z_K}{\partial x_1,\dots,x_{K-1},v} \right\rvert 
&=\left\lvert \frac{\partial vx_1, \dots,vx_{K-1}, v(1-\sum_{k=1}^{K-1}x_k)}{\partial x_1,\dots,x_{K-1},v} \right\rvert \\

&= 
\begin{vmatrix}
v & 0 & \cdots & 0 & x_1 \\ 
0 & \ddots & \ddots & \vdots & \vdots \\
\vdots & \ddots & \ddots & 0 &\vdots \\
0 & \cdots & 0 & v & x_{K-1} \\
-v & \cdots & \cdots & -v & 1-\sum_{k=1}^{K-1}x_k
\end{vmatrix}
= 
\begin{vmatrix}
v & 0 & \cdots & 0 & x_1 \\ 
0 & \ddots & \ddots & \vdots & \vdots \\
\vdots & \ddots & \ddots & 0 &\vdots \\
0 & \cdots & 0 & v & x_{K-1} \\
0 & \cdots & \cdots & 0 & 1 
\end{vmatrix}
=v^{K-1}.
\end{align*}
Thus, by the transformation formula we get the joint distribution
\begin{align}
f_{X,V}(x_1,\dots,x_{K-1},v)
&= f_Z(vx_1,\dots,vx_{K-1},v(1-\sum_{k=1}^{K-1}x_k)) \dot{} \left\lvert \frac{\partial vx_1, \dots,vx_{K-1}, v(1-\sum_{k=1}^{K-1}x_k)}{\partial x_1,\dots,x_{K-1},v} \right\rvert \\
&= \frac{1}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}(vx_k)^{\alpha_k-1} \dot{} (v(1-\prod_{k=1}^{K-1}vx_k))^{\alpha_K-1} \dot{} e^{-\sum_{k=1}^Kvx_k} \dot{} v^{K-1} \\
&= \frac{v^{\sum_{k=1}^{K-1}(\alpha_i-1)+(\alpha_K-1)+(K-1)}e^{-v}}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1} \\
&= \frac{v^{(\sum_{k=1}^{K}\alpha_i)-1}e^{-v}}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1}
\end{align}
And by integration the marginal distribution of $(x_1,\dots,x_{K-1})$ is
$$
f_X(x_1,\dots,x_{K-1})=  \frac{\int_0^\infty v^{(\sum_{k=1}^{K}\alpha_i)-1}e^{-v}dv}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1}
$$
$$
=f_X(x_1,\dots,x_{K-1})=  \frac{\Gamma(\sum_{k=1}^{K}\alpha_i)}{\prod_{k=1}^K\Gamma(\alpha_k)}\dot{} \prod_{k=1}^{K-1}x_k^{\alpha_k-1} \dot{} (1-\prod_{k=1}^{K-1}x_k)^{\alpha_K-1}
$$
which is the Dirichlet distribution.

##2)

```{r}

#### FEIL GAMMAFUNKSJON
random.dirichlet <- function(alpha){
  K <- length(alpha)
  z <- rgamma(K,alpha,1) #n,shape,rate
  v <- sum(z)
  return (z/v)
}

N_dir <- 1000
alpha <- c(.1, .5, .1, .3, .8)
K <- length(alpha)
dirichlet.sample <- matrix(NA,N_dir,K)
for (i in 1:N_dir){
  dirichlet.sample[i,] <- random.dirichlet(alpha)
}
#Empirical mean and variance
colMeans(dirichlet.sample)
diag(var(dirichlet.sample))

alphasum <- sum(alpha)
#Analytical mean and variance
alpha1 <- alpha/alphasum
alpha1
alpha1*(1-alpha1)/(1+alphasum)
```

#Problem D: Rejection sampling and importance sampling

##1)
```{r}
random.multinomial <- function(n){
  x <- runif()
  u <- runif()
  
}
```

##2)

##3)

##4)