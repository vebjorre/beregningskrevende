---
title: "TMA4300 - Exercise 2"
author: "Camilla og Vebj√∏rn"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A: The coal-mining disaster data
In this problem we are going to analyse a data set of time intervals between successive coal-mining disasters in the UK involving ten or more men killed in the periode March 15th 1851 to March 22nd 1962. 

## 1) 
We start by making a plot with the cumulative number of disasters along the y-axis and year along the x-axis. 
```{r}
library(boot)
data <- coal
plot(data$date, 1:191,xlab="year",ylab="cumulative number of disasters") #remove first and last
```

From the plot we can see that the cumulative number of disasters increases almost linearly until around 1900 and that the ...

Hence, we see that the incidence of coal-mining disasters was more frequent in the first hundred years and that the frequent of disasters become much smaller after around year 1945.

## 2) 
For $n=1$ we are given the likelihood 
$$
f(x|t_1,\lambda_0,\lambda_1)=\lambda_0^{y_0}\lambda_1^{y_1}exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)),
$$
where $x$ is the observed data and $y_k$ is the number of observed disasters in the period $t_k$ to $t_{k+1}$. The priors $f(t_1)\sim \textrm{unif}(t_0,t_2)$ and $f(\lambda_i|\beta) \sim \textrm{gamma}(\alpha=2,\beta)$, for $\lambda_i=1,2$, are independent. For the hyper prior we use $f(\beta)\propto exp(-1/\beta) / \beta$.

Hence, the posterior distribution for $\theta=(t_1,\lambda_0,\lambda_1,\beta)$ given $x$ up to a normalising constant is given by
$$
f(\theta|x)=\frac{f(\theta,x)}{f(x)}\propto f(\theta,x)=f(t_1)f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1)
\propto f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1).
$$
By inserting the likelihood and the priors we get the posterior distribution 
$$ 
f(\theta|x)\propto
\frac{1}{\beta^5}\lambda_0^{1+y_0}\lambda_1^{1+y_1}exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\right).
$$

## 3) 
To find the full conditionals for each of the elements in $\theta$ we omit all multiplicative factors in $f(\theta|x)$ that do not depend on the element. Thus we get the non normalised conditionals
$$
f(t_1|\lambda_0,\lambda_1,\beta,x)\propto exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1))=exp(-t_1(\lambda_0-\lambda_1)),
$$
$$
f(\lambda_0|t_1,\lambda_1,\beta,x)\propto \lambda_0^{1+y_0} exp\left(-\lambda_0(1/\beta+t_1-t_0)\right),
$$
$$
f(\lambda_1|t_1,\lambda_0,\beta,x)\propto \lambda_1^{1+y_1} exp\left(-\lambda_1(1/\beta+t_2-t_1)\right),
$$
$$
f(\beta|t_1,\lambda_0,\lambda_1,x)\propto \frac{1}{\beta^5}exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)\right).
$$
From this we can see that $t_1$ belongs to the exponential distribution with $\beta=(\lambda_0-\lambda_1)^{-1}$ and that $\lambda_i$ belongs to the gamma distribution with the shape parameter $\alpha=2+y_i$ and scale parameter $\beta=(1/\beta+t_{i+1}-t_i)^{-1}$ for $i=1,2$.

## 4) 

## 5) 

## 6) 

## 7) 

## 8) 

# Problem B: INLA for Gaussian Data
We start by plotting the dataset. 
```{r}
data=read.table("https://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/exercise2/Gaussiandata.txt")
names(data)="y"
plot(data$y)
```

## 1)
A latent Gaussian model, LGM, consists of three elements: a likelihood model, a latent Gaussian field and a vector of hyperparameters. In our model the observations $y_t$ are assumed independent and Gaussian distributed with mean $\eta_t$ and known unit varuance. Hence, the likelihood model is given as $y_t|\eta_t=\mathcal{N}(\eta_t,1); t=1,...T$. The linear predictor $\eta_t$ is linked to a smooth effect of time $t$ as $\eta_t=f_t$. For the vector $\mathbf{f}=(f_1,\dots, f_T)$ we have the second order random walk as the prior distribution, such that
$$
\pi(\mathbf{f}|\theta) \propto \theta^{(T-2)/2}exp\left\{\frac{\theta}{2}\sum_{t=3}^T[f_t-2f_{t-1}+f_{t-2}]^2 \right\} = \mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}).
$$
Hence, the latent field $\mathbf{f}$ is a Gaussian Markov random field, GMRF, with sparce precision matrix $\mathbf{Q}(\theta)^{-1}$. The precision parameter $\theta$ controls the smoothness of the vector $\mathbf{f}$ and is our hyperparameter, $\theta \sim \textrm{gamma}(1,1)$.

It is possible to use INLA to estimate the parameters because our inferential interest lies in the posterior marginal for the smooth effect $\pi(\eta_t|\mathbf{y})$,$t=1,\dots,T$, and the LGM fulfill the following assumtions. Each data point $y_t$ depends only on one of the elements in the latent Gausian field $\mathbf{f}$, the linear predictor $\eta_t$. The hyperparameter vector should be small, and in our case it contains only one parameter $\theta$. The precion matrix $\mathbf{Q}(\theta)^{-1}$ is sparse, and the linear predictor depends linearly on the unkown smooth function of temporal effects.  

## 2) 
```{r}
library(MASS)
nsamples=100
y=data$y

beta <- function(eta,nsamples){ 
  sum <- 0
  for (t in 3:nsamples){
    sum <- sum + (eta[t]-2*eta[t-1]+eta[t-2])^2
  }
  return (1+0.5*sum)
}

block_Gibbs <- function(y, theta, eta, alpha = 0.001, beta = 0.001, nsamples = 5000) 
{
  theta <- rep(0,nsamples)
  eta <- rep(0,nsamples)
  
  theta[1] <- rgamma(1,1,1)  #initial values
  eta[1] <- rep(mean(y)) #initial values
  count <- 0
  for (i in 1:nsamples) { 
    ## update theta
    scale=beta(eta,nsamples)
    new_theta <- rgamma(1,shape=nsamples/2,scale=scale)
    ## update the vector eta with the new theta
    new_eta <- 
    
    theta[i] = new_theta
    eta[i]=new_eta
      
  }
  return(data.frame(theta=theta, eta=eta))
}

```

## 3) 


## 4) 


## 5) 
```{r}
#install.packages("INLA", repos=c(getOption("repos"),
#INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
library("INLA")
x=1:20
df=data.frame(y=data$y,x=x)

# specify the prior
my.hyper <- list(theta = list(prior="gamma", param=c(1,1)))
# specify the linear predictor
formula <- y ~ -1 + f(x, model = "rw2", hyper = my.hyper)
#Fit the model
# result <- inla(formula=formula, data=df, family="gaussian",control.family = list(hyper = my.hyper))
# 
# plot(result$marginals.hyperpar$`Precision for x`, type="l")
# summary(result)

```

