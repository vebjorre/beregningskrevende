---
title: "TMA4300 - Exercise 2"
author: "Camilla og Vebj√∏rn"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A: The coal-mining disaster data
In this problem we are going to analyse a data set of time intervals between successive coal-mining disasters in the UK involving ten or more men killed in the periode March 15th 1851 to March 22nd 1962. 

## 1) 
We start by making a plot with the cumulative number of disasters along the y-axis and year along the x-axis. 
```{r}
library(boot)
data <- coal
plot(data$date, 1:191,xlab="year",ylab="cumulative number of disasters") #remove first and last
```

From the plot we can see that the cumulative number of disasters increases almost linearly until around 1900 and that the ...

Hence, we see that the incidence of coal-mining disasters was more frequent in the first hundred years and that the frequent of disasters become much smaller after around year 1945.

## 2) 
For $n=1$ we are given the likelihood 
$$
f(x|t_1,\lambda_0,\lambda_1)=\lambda_0^{y_0}\lambda_1^{y_1}exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)),
$$
where $x$ is the observed data and $y_k$ is the number of observed disasters in the period $t_k$ to $t_{k+1}$. The priors $f(t_1)\sim \textrm{unif}(t_0,t_2)$ and $f(\lambda_i|\beta) \sim \textrm{gamma}(\alpha=2,\beta)$, for $\lambda_i=1,2$, are independent. For the hyper prior we use $f(\beta)\propto exp(-1/\beta) / \beta$.

Hence, the posterior distribution for $\theta=(t_1,\lambda_0,\lambda_1,\beta)$ given $x$ up to a normalising constant is given by
$$
f(\theta|x)=\frac{f(\theta,x)}{f(x)}\propto f(\theta,x)=f(t_1)f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1)
\propto f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1).
$$
By inserting the likelihood and the priors we get the posterior distribution 
$$ 
f(\theta|x)\propto
\frac{1}{\beta^5}\lambda_0^{1+y_0}\lambda_1^{1+y_1}exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\right).
$$

## 3) 
To find the full conditionals for each of the elements in $\theta$ we omit all multiplicative factors in $f(\theta|x)$ that do not depend on the element. Thus we get the non normalised conditionals
$$
f(t_1|\lambda_0,\lambda_1,\beta,x)\propto exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1))=exp(-t_1(\lambda_0-\lambda_1)),
$$
$$
f(\lambda_0|t_1,\lambda_1,\beta,x)\propto \lambda_0^{1+y_0} exp\left(-\lambda_0(1/\beta+t_1-t_0)\right),
$$
$$
f(\lambda_1|t_1,\lambda_0,\beta,x)\propto \lambda_1^{1+y_1} exp\left(-\lambda_1(1/\beta+t_2-t_1)\right),
$$
$$
f(\beta|t_1,\lambda_0,\lambda_1,x)\propto \frac{1}{\beta^5}exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)\right).
$$
From this we can see that $t_1$ belongs to the exponential distribution with $\beta=(\lambda_0-\lambda_1)^{-1}$ and that $\lambda_i$ belongs to the gamma distribution with the shape parameter $\alpha=2+y_i$ and scale parameter $\beta=(1/\beta+t_{i+1}-t_i)^{-1}$ for $i=1,2$.

## 4) 
```{r}
library(boot)
library(MASS)
date <- coal$date
t0 <- date[1]
t2 <- date[191]
date <- date[-c(1,191)]

target <- function(x){
  return (1/x[4]^5 * exp(-(1+x[2]+x[3])/x[4]) * x[2]^(x[5]+1) * x[3]^(189-x[5]+1) * exp(x[1]*(x[3]-x[2]) + x[2]*t0 - x[3]*t2))
}

fullcond <- function(x,r){
  if (r==1){
    return (dexp(x[1], rate=1/(x[1]-x[2])))
  }
  else if (r==2){
    return (dgamma(x[2], shape=x[5]+2, scale=1/(1/x[4]+x[1]-t0)))
  }
  else if (r==3){
    return (dgamma(x[3], shape=189-x[5]+2, scale=1/(1/x[4]+t2-x[1])))
  }
  else if (r==4){
    return (1/x[4]^5 * exp(-1/x[4]*(1+x[2]+x[3])))
  }
}

fullcond.b1 <- function(x){
  return( x[2]^(x[5]+1) * x[3]^(189-x[5]+1) * exp(-(1+x[2]+x[3])/x[4] + x[1]*(x[3]-x[2]) + x[2]*t0 - x[3]*t2) )
}

mcmc_RW <- function(d, ntimes = 1000)
{
  x <- matrix(nrow=ntimes, ncol=5)
  x[1,1] <- runif(1,t0,t2)
  x[1,2:4] <- runif(3,0,1)
  x[1,5] <- sum(date<x[1,1])
  for(i in 2:ntimes)
  {
    for (j in 1:4){
      y <- runif(1,x[i-1,j]-d[j],x[i-1,j]+d[j])
      temp <- x[i-1,]
      temp[j] <- y
      alpha <- min(1,fullcond(temp,j)/fullcond(x[i-1,],j), na.rm=TRUE)
      # cat("alpha:",alpha, "j:",j, "y:",y, "x:",x[i-1,], "\n")
      if (runif(1)<alpha){
        if (j==1){
          if (t0 <= y && y <= t2){
            x[i,j] = y
          }
          else{
            x[i,j] = x[i-1,j]
          }
        }
        else{
          x[i,j] <- y
        }
        
      }
      else{
        x[i,j] <- x[i-1,j]
      }
    }
    x[i,5] <- sum(date<=x[i,1])
  }
  return(x)
}
```

## 5) 
```{r}

```

## 6) 
```{r}

```
## 7) 
```{r}
mcmc_block <- function(d, ntimes)
{
  x <- matrix(nrow=ntimes, ncol=5)
  x[1,1] <- runif(1,t0,t2)
  x[1,2:4] <- runif(3,0,1)
  x[1,5] <- sum(date<x[1,1])
  for(i in 2:ntimes)
  {
    t1 <- rnorm(1,mean=x[i-1,1],sd=.5)
    y0 <- sum(date<t1)
    lam0 <- runif(1, x[i-1,2]-d[2], x[i-1,2]+d[2])
    lam1 <- runif(1, x[i-1,3]-d[3], x[i-1,3]+d[3])
    temp <- x[i-1,]
    temp[1] <- t1
    temp[2] <- lam0
    temp[3] <- lam1
    temp[5] <- y0
    alpha <- min(1,fullcond.b1(temp)/fullcond.b1(x[i-1,]), na.rm=TRUE)
    # cat("alpha:",alpha, "x:",x[i-1,], "\n")
    if (runif(1)<alpha){
      x[i,] <- temp
    }
    else{
      x[i,] <- x[i-1,]
    }
    
    temp <- x[i,]
    beta <- max(0,rnorm(1,mean=temp[4], sd=.5))
    temp[4] <- beta
    lam0 <- runif(1, x[i,2]-d[2], x[i,2]+d[2])
    lam1 <- runif(1, x[i,3]-d[3], x[i,3]+d[3])
    temp[2] <- lam0
    temp[3] <- lam1
    alpha <- min(1, fullcond.b1(temp)/fullcond.b1(x[i,]), na.rm=TRUE)
    cat("alpha:",alpha, "x:",x[i-1,], "\n")
    if (runif(1)<alpha){
      x[i,] <- temp
    }
  }
  return(x)
}
```

## 8) 
```{r}
d <- c(50,3,3,3)
d.b <- c(1,.1,.1,.1)
ntimes <- 10000


x.b <- mcmc_block(d.b,ntimes)

par(mfrow=c(2,2))
plot(x.b[,1], type='l')
plot(x.b[,2], type='l')
plot(x.b[,3], type='l')
plot(x.b[,4], type='l')
```

# Problem B: INLA for Gaussian Data
We start by plotting the dataset. 
```{r}
data=read.table("https://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/exercise2/Gaussiandata.txt")
names(data)="y"
plot(data$y)
```

## 1)
A latent Gaussian model, LGM, consists of three elements: a likelihood model, a latent Gaussian field and a vector of hyperparameters. In our model the observations $y_t$ are assumed independent and Gaussian distributed with mean $\eta_t$ and known unit varuance. Hence, the likelihood model is given as $y_t|\eta_t=\mathcal{N}(\eta_t,1); t=1,...T$. The linear predictor $\eta_t$ is linked to a smooth effect of time $t$ as $\eta_t=f_t$. For the vector $\mathbf{f}=(f_1,\dots, f_T)$ we have the second order random walk as the prior distribution, such that
$$
\pi(\mathbf{f}|\theta) \propto \theta^{(T-2)/2}exp\left\{\frac{\theta}{2}\sum_{t=3}^T[f_t-2f_{t-1}+f_{t-2}]^2 \right\} = \mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}).
$$
Hence, the latent field $\mathbf{f}$ is a Gaussian Markov random field, GMRF, with sparce precision matrix $\mathbf{Q}(\theta)^{-1}$. The precision parameter $\theta$ controls the smoothness of the vector $\mathbf{f}$ and is our hyperparameter, $\theta \sim \textrm{gamma}(1,1)$.

It is possible to use INLA to estimate the parameters because our inferential interest lies in the posterior marginal for the smooth effect $\pi(\eta_t|\mathbf{y})$,$t=1,\dots,T$, and the LGM fulfill the following assumtions. Each data point $y_t$ depends only on one of the elements in the latent Gausian field $\mathbf{f}$, the linear predictor $\eta_t$. The hyperparameter vector should be small, and in our case it contains only one parameter $\theta$. The precion matrix $\mathbf{Q}(\theta)^{-1}$ is sparse, and the linear predictor depends linearly on the unkown smooth function of temporal effects.  

## 2) 
```{r}
library(MASS)
library(Matrix)
library(mvtnorm)

nsamples=100
y=data$y

beta <- function(eta,nsamples,T){ 
  sum <- 0
  for (t in 3:T){
    sum <- sum + (eta[t]-2*eta[t-1]+eta[t-2])^2
  }
  return (1+0.5*sum)
}

Q_matrix <- function(T){
  Q <- diag(1,T,T)
  Q[row(Q) - col(Q) == 1] <- -2
  Q[row(Q) - col(Q) == 2] <- 1
  return(Q%*%t(Q))
}

block_Gibbs <- function(y, nsamples) 
{
  T=length(y)
  Q <- Q_matrix(T)
  theta <- rep(0,nsamples)
  eta <- matrix(0,nsamples,T)
  
  theta[1] <- rgamma(1,1,1)  #initial values
  eta[1,] <- y#rep(mean(y),T) #initial values
  count <- 0
  for (i in 2:nsamples) { 
    ## update theta
    rate=1+0.5*t(eta[i-1,])%*%Q%*%eta[i-1,]
    theta[i] <- rgamma(1,shape=T/2,rate=rate)
    ## update the vector eta with the new theta
    I=diag(1,T)
    var=solve(Q*theta[i]+I,sparse=TRUE)
    mean=var%*%y
    eta[i,] <- rmvnorm(mean, var)
      
  }
  list=cbind(theta=theta, eta=eta) 
  return(list)
}
n=10000
list=block_Gibbs(y,n)

plot(list[n,-1])

```

## 3) 
```{r}
theta <- seq(0,1.5,0.1)

```

## 4) 
```{r}

```


## 5) 
```{r}
#install.packages("INLA", repos=c(getOption("repos"),
#INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
library("INLA")
x=(1:20)
df=data.frame(y=data$y,x=x)

# specify the prior
my.hyper <- list(theta = list(prior="log.gamma", param=c(1,1)))
# specify the linear predictor
formula <- y ~ -1 + f(x, model = "rw2", hyper = my.hyper)
#Fit the model
result <- inla(formula=formula, family="gaussian", data=df,verbose = FALSE) 
#,control.family = list(hyper = my.hyper),control.compute = list(config=TRUE)

plot(result$summary.random$x$mean)
summary(result)
```

