---
title: "TMA4300 - Exercise 2"
author: "Camilla og Vebj√∏rn"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A: The coal-mining disaster data
In this problem we are going to analyse a data set of time intervals between successive coal-mining disasters in the UK involving ten or more men killed in the period March 15th 1851 to March 22nd 1962. 

## 1) 
We start by making a plot with the cumulative number of disasters along the y-axis and year along the x-axis. 
```{r}
library(boot)
date <- coal$date #fetch data
t0 <- date[1] #start date
t2 <- date[191] #end date
date <- date[-c(1,191)] #data
plot(date, 1:189, xlab="year", ylab="cumulative number of disasters", sub="Cumulative number of coal-mining disasters in UK from 1851 to 1962")
```

From the plot we can see that the cumulative number of disasters increases almost linearly until around 1900 and that the ...

Hence, we see that the incidence of coal-mining disasters was more frequent in the first hundred years and that the frequency of disasters becomes much smaller after around year 1945.

## 2) 
For $n=1$ we are given the likelihood 
$$
f(x|t_1,\lambda_0,\lambda_1)=\lambda_0^{y_0}\lambda_1^{y_1}exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)),
$$
where $x$ is the observed data and $y_k$ is the number of observed disasters in the period $t_k$ to $t_{k+1}$. The priors $f(t_1)\sim \textrm{unif}(t_0,t_2)$ and $f(\lambda_i|\beta) \sim \textrm{gamma}(\alpha=2,\beta)$, for $\lambda_i=1,2$, are independent. For the hyper prior we use $f(\beta)\propto exp(-1/\beta) / \beta$.

Hence, the posterior distribution for $\theta=(t_1,\lambda_0,\lambda_1,\beta)$ given $x$ up to a normalising constant is given by
$$
f(\theta|x)=\frac{f(\theta,x)}{f(x)}\propto f(\theta,x)=f(t_1)f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1)
\propto f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1).
$$
By inserting the likelihood and the priors we get the posterior distribution 
$$ 
f(\theta|x)\propto
\frac{1}{\beta^5}\lambda_0^{1+y_0}\lambda_1^{1+y_1}exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\right).
$$

## 3) 
To find the full conditionals for each of the elements in $\theta$ we omit all multiplicative factors in $f(\theta|x)$ that do not depend on the element. Realizing $y_0$ and $y_1$ are functions of $t_1$ we get the non normalised conditionals
$$
f(t_1|\lambda_0,\lambda_1,\beta,x)\propto \lambda_0^{1+y_0}\lambda_1^{1+y_1}\exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)) \propto \lambda_0^{1+y_0}\lambda_1^{1+y_1}\exp(-t_1(\lambda_0-\lambda_1)),
$$
$$
f(\lambda_0|t_1,\lambda_1,\beta,x)\propto \lambda_0^{1+y_0} \exp\left(-\lambda_0(1/\beta+t_1-t_0)\right),
$$
$$
f(\lambda_1|t_1,\lambda_0,\beta,x)\propto \lambda_1^{1+y_1} \exp\left(-\lambda_1(1/\beta+t_2-t_1)\right),
$$
$$
f(\beta|t_1,\lambda_0,\lambda_1,x)\propto \frac{1}{\beta^5}\exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)\right).
$$
From this we can see that $\lambda_i$ belongs to the gamma distribution with shape parameter $\alpha=2+y_i$ and rate parameter $\theta=1/\beta+t_{i+1}-t_i$ for $i=1,2$ and that $\beta$ comes from the inverse gamma distribution with shape $\alpha=4$ and rate $\theta=1+\lambda_0+\lambda_2$.

## 4)
We now implement a single site MCMC algorithm for $f(\theta|x)$. Since the full conditionals for $\lambda_0$, $\lambda_1$ and $\beta$ are known distributions we can just draw new values from these distributions. For $t_1$ we use a random walk proposal, namely the normal distribution with mean $t_1^{(i-1)}$ and standard deviation $\sigma$. Here $\sigma$ is a tuning parameter we can use to configure the Markov chain. Since this is a symmetric proposal the acceptance probability for the potential new value $\tilde t_1$ is then given by 
$$
\min\{1,\frac{f(\tilde t_1|\lambda_0,\lambda_1,\beta,x)}{f(t_1|\lambda_0,\lambda_1,\beta,x)}\} = \min\{1, \frac{\lambda_0^{1+\tilde y_0}\lambda_1^{1+\tilde y_1}}{\lambda_0^{1+y_0}\lambda_1^{1+y_1}}\exp((\tilde t_1-t_1)(\lambda_0-\lambda_1)) \}.
$$
where all parameters parameters not marked with "~" are taken from $\theta^{(i-1)}$ and $\tilde y_0$ and $\tilde y_1$ are the updated values given $\tilde t_1$. We also need to keep sure $t_0<t_1<t_2$. We enforce this by setting $\alpha=0$ when the proposed value $\tilde t_1$ does not satisfy this constraint. As initial values we draw $t_1^{(0)}$ from $unif(t_0,t_2)$ and the rest from $unif(0,1)$.

```{r}
library(MASS)
library(invgamma)

logtarget <- function(x){ # x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  return (-5*log(x[4]) + (x[5]+1)*log(x[2]) + (189-x[5]+1)*log(x[3]) - (1+x[2]+x[3])/x[4] + x[1]*(x[3]-x[2]) + x[2]*t0 - x[3]*t2 )
}

fullcond <- function(x,r){ #x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  if (r==1){ #LOGSCALE!
    return( (x[5]+1)*log(x[2]) + (189-x[5]+1)*log(x[3]) + x[1]*(x[3]-x[2]) )
  }
  if (r==2){
    return (rgamma(1, shape=x[5]+2, rate=1/x[4]+x[1]-t0))
  }
  else if (r==3){
    return (rgamma(1, shape=189-x[5]+2, rate=1/x[4]+t2-x[1]))
  }
  else if (r==4){
    return (rinvgamma(1, shape=4, rate=1+x[2]+x[3]))
  }
}

mcmc_RW <- function(sd, ntimes = 1000){ #x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  x <- matrix(nrow=ntimes, ncol=5)
  x[1,1] <- runif(1,t0,t2)
  x[1,2:4] <- runif(3,0,1)
  x[1,5] <- sum(date<x[1,1])
  for(i in 2:ntimes){
    x[i,2] <- fullcond(x[i-1,],2)
    x[i,3] <- fullcond(x[i-1,],3)
    x[i,4] <- fullcond(x[i-1,],4)
    temp <- x[i-1,]
    t1 <- rnorm(1, temp[1], sd)
    temp[1] <- t1
    temp[5] <- sum(date<t1)
    alpha <- min(1,exp(fullcond(temp,1)-fullcond(x[i-1],1)), na.rm=TRUE)
    if (runif(1)<alpha && t0<t1 && t1<t2){
      x[i,1] <- t1
      x[i,5] <- sum(date<=t1)
      }
    else{
      x[i,1] <- x[i-1,1]
      x[i,5] <- x[i-1,5]
      }
  }
  return(x)
}
```

## 5)
```{r}
sd <- 1
ntimes <- 100000

x1 = mcmc_RW(sd, ntimes)

par(mfrow=c(2,2))
plot(x1[,1], type='l', xlab="Iteration", ylab="t1")
plot(x1[,2], type='l', xlab="Iteration", ylab="lambda0")
plot(x1[,3], type='l', xlab="Iteration", ylab="lambda1")
plot(x1[,4], type='l', xlab="Iteration", ylab="beta")

burnin <- c(1:1000)

par(mfrow=c(2,2))
truehist(x1[burnin,1], xlab="t1")
truehist(x1[burnin,2], xlab="lambda0")
truehist(x1[burnin,3], xlab="lambda1")
truehist(x1[burnin,4], xlim=c(0,10), xlab="beta")

par(mfrow=c(2,2))
acf(x1[burnin,1])
acf(x1[burnin,2])
acf(x1[burnin,3])
acf(x1[burnin,4])

mean(x1[-burnin,1])
mean(x1[-burnin,2])
mean(x1[-burnin,3])
mean(x1[-burnin,4])

```

## 6) 
```{r}

```
## 7) 
Instead of single site updates we now do two block proposals. In the first block we keep $\beta$ unchanged and start by generating $\tilde t_1$ from a normal distribution with the current value of $t_1$ as mean and standard deviation $\sigma_1$. We then generate $\lambda_0$ and $\lambda_1$ from their joint full conditional given $\tilde t_1$, i.e. $f(\lambda_0, \lambda_1 | x, \tilde t_1, \beta)$. We observe that this joint density is the product of their marginal full conditionals and thus they are independent and can be generated from their marginals, given $\tilde t_1$. This block update is accepted with the usual Metropolis-Hastings acceptance probability, but with three new values instead of one.
The second block is identical to the first only switching the roles of $t_1$ and $\beta$ and using $\sigma_2$ as standard deviation in the proposal for $\beta$.

```{r}
mcmc_block <- function(sd, ntimes)
{
  x <- matrix(nrow=ntimes, ncol=5)
  x[1,1] <- runif(1,t0,t2)
  x[1,2:4] <- runif(3,0,1)
  x[1,5] <- sum(date<x[1,1])
  for(i in 2:ntimes)
  {
    t1 <- rnorm(1,mean=x[i-1,1],sd=sd[1])
    if (t0<t1 && t1<t2){
      y0 <- sum(date<t1)
      temp <- x[i-1,]
      temp[1] <- t1
      temp[5] <- y0
      lam0 <- fullcond(temp,2)
      lam1 <- fullcond(temp,3)
      temp[2] <- lam0
      temp[3] <- lam1
      alpha <- min(1,exp(logtarget(temp)-logtarget(x[i-1,])), na.rm=TRUE)
      if (runif(1)<alpha){
        x[i,] <- temp
      }
      else{
        x[i,] <- x[i-1,]
      }
    }
    else{
      x[i,] <- x[i-1,]
    }
    
    temp <- x[i,]
    beta <- rnorm(1,mean=temp[4], sd=sd[2])
    if (beta>0){
      temp[4] <- beta
      lam0 <- fullcond(temp,2)
      lam1 <- fullcond(temp,3)
      temp[2] <- lam0
      temp[3] <- lam1
      alpha <- min(1,exp(logtarget(temp)-logtarget(x[i-1,])),na.rm=TRUE)
      if (runif(1) < alpha){
        x[i,] <- temp
      }
    }
  }
  return(x)
}
```

## 8)
In this block Metropolis-Hastings algorithm we have two tuning parameters, $\sigma=(\sigma_1,\sigma_2)$. ...
```{r}
sd.b <- c(1,2)
ntimes <- 50000

x.b <- mcmc_block(sd.b,ntimes)

par(mfrow=c(2,2))
plot(x.b[,1], type='l')
plot(x.b[,2], type='l')
plot(x.b[,3], type='l')
plot(x.b[,4], type='l')

burnin <- c(1:1000)

par(mfrow=c(2,2))
truehist(x.b[-burnin,1])
truehist(x.b[-burnin,2])
truehist(x.b[-burnin,3])
truehist(x.b[-burnin,4], xlim=c(0,10))

par(mfrow=c(2,2))
acf(x.b[-burnin,1])
acf(x.b[-burnin,2])
acf(x.b[-burnin,3])
acf(x.b[-burnin,4])

mean(x.b[-burnin,1])
mean(x.b[-burnin,2])
mean(x.b[-burnin,3])
mean(x.b[-burnin,4])

cov(x.b[-burnin,2], x.b[-burnin,3])
```

# Problem B: INLA for Gaussian Data
We start by plotting the dataset. 
```{r}
data=read.table("https://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/exercise2/Gaussiandata.txt")
names(data)="y"
plot(data$y)
```

## 1)
A latent Gaussian model, LGM, consists of three elements: a likelihood model, a latent Gaussian field and a vector of hyperparameters. In our model the observations $y_t$ are assumed independent and Gaussian distributed with mean $\eta_t$ and known unit varuance. Hence, the likelihood model is given as $y_t|\eta_t=\mathcal{N}(\eta_t,1); t=1,...T$. The linear predictor $\eta_t$ is linked to a smooth effect of time $t$ as $\eta_t=f_t$. For the vector $\mathbf{f}=(f_1,\dots, f_T)$ we have the second order random walk as the prior distribution, such that
$$
\pi(\mathbf{f}|\theta) \propto \theta^{(T-2)/2}exp\left\{\frac{\theta}{2}\sum_{t=3}^T[f_t-2f_{t-1}+f_{t-2}]^2 \right\} = \mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}).
$$
Hence, the latent field $\mathbf{f}$ is a Gaussian Markov random field, GMRF, with sparce precision matrix $\mathbf{Q}(\theta)^{-1}$. The precision parameter $\theta$ controls the smoothness of the vector $\mathbf{f}$ and is our hyperparameter, $\theta \sim \textrm{gamma}(1,1)$.

It is possible to use INLA to estimate the parameters because our inferential interest lies in the posterior marginal for the smooth effect $\pi(\eta_t|\mathbf{y})$,$t=1,\dots,T$, and the LGM fulfill the following assumtions. Each data point $y_t$ depends only on one of the elements in the latent Gausian field $\mathbf{f}$, the linear predictor $\eta_t$. The hyperparameter vector should be small, and in our case it contains only one parameter $\theta$. The precion matrix $\mathbf{Q}(\theta)^{-1}$ is sparse, and the linear predictor depends linearly on the unkown smooth function of temporal effects.  

## 2) 
```{r}
library(MASS)
library(Matrix)
library(dae)
library(matrixStats)

Q_matrix <- function(T){
  Q <- diag(1,T,T)
  Q[row(Q) - col(Q) == 1] <- -2
  Q[row(Q) - col(Q) == 2] <- 1
  QQ=Q%*%t(Q)
  QQ[T,T]=1
  QQ[T-1,T-1]=5
  QQ[T-1,T]=QQ[T,T-1]=-2
  return(QQ)
}

block_Gibbs <- function(y, nsamples) 
{
  T=length(y)
  Q <- Q_matrix(T)
  theta <- rep(0,nsamples)
  eta <- matrix(0,nsamples,T)
  
  theta[1] <- rgamma(1,1,1)  #initial values
  eta[1,] <- y #initial values
  count <- 0
  for (i in 2:nsamples) { 
    ## update theta
    rate=1+0.5*t(eta[i-1,])%*%Q%*%eta[i-1,]
    theta[i] <- rgamma(1,shape=T/2,rate=rate)
    ## update the vector eta with the new theta
    I=diag(1,T)
    var=solve(Q*theta[i]+I,sparse=TRUE)
    mean=var%*%y
    eta[i,] <- rmvnorm(mean, var)
      
  }
  list=cbind(theta=theta, eta=eta) 
  return(list)
}

n=10000
y=data$y

list=block_Gibbs(y,n)
t=seq(1,20,1)

m <- colMeans(list[-c(1:1000),])
v <- colSds(list[-c(1:1000),])
m_upper <- m+qnorm(0.05)*v
m_lower <- m-qnorm(0.05)*v

plot(t,y)
lines(t,m[-1])
lines(t,m_upper[-1])
lines(t,m_lower[-1])

truehist(list[,1])

```

## 3) 
```{r}
theta <- seq(0,1.5,0.1)

```

## 4) 
```{r}

```


## 5) 
```{r}
#install.packages("INLA", repos=c(getOption("repos"),
#INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
library("INLA")
x=(1:20)
df=data.frame(y=data$y,x=x)

# specify the prior
my.hyper <- list(theta = list(prior="log.gamma", param=c(1,1)))
# specify the linear predictor
formula <- y ~ -1 + f(x, model = "rw2", hyper = my.hyper)
#Fit the model

result <- inla(formula=formula, family="gaussian", data=df,verbose = FALSE) 
#,control.family = list(hyper = my.hyper),control.compute = list(config=TRUE)

plot(result$summary.random$x$mean)
plot(result$marginals.hyperpar$`Precision for the Gaussian observations`,xlim=c(0,6))
summary(result)
```

