---
title: "TMA4300 - Exercise 2"
author: "Camilla og Vebj√∏rn"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A: The coal-mining disaster data
In this problem we are going to analyse a data set of time intervals between successive coal-mining disasters in the UK involving ten or more men killed in the period March 15th 1851 to March 22nd 1962. 

## 1) 
We start by making a plot with the cumulative number of disasters along the y-axis and year along the x-axis. 
```{r}
library(boot)
date <- coal$date #fetch data
t0 <- date[1] #start date
t2 <- date[191] #end date
date <- date[-c(1,191)] #data
plot(date, 1:189, xlab="year", ylab="cumulative number of disasters", sub="Cumulative number of coal-mining disasters in UK from 1851 to 1962")
```

From the plot we can see that the cumulative number of disasters increases almost linearly until around 1900 and that the ...

Hence, we see that the incidence of coal-mining disasters was more frequent in the first hundred years and that the frequency of disasters becomes much smaller after around year 1945.

## 2) 
For $n=1$ we are given the likelihood 
$$
f(x|t_1,\lambda_0,\lambda_1)=\lambda_0^{y_0}\lambda_1^{y_1}exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)),
$$
where $x$ is the observed data and $y_k$ is the number of observed disasters in the period $t_k$ to $t_{k+1}$. The priors $f(t_1)\sim \textrm{unif}(t_0,t_2)$ and $f(\lambda_i|\beta) \sim \textrm{gamma}(\alpha=2,\beta)$, for $\lambda_i=1,2$, are independent. For the hyper prior we use $f(\beta)\propto exp(-1/\beta) / \beta$.

Hence, the posterior distribution for $\theta=(t_1,\lambda_0,\lambda_1,\beta)$ given $x$ up to a normalising constant is given by
$$
f(\theta|x)=\frac{f(\theta,x)}{f(x)}\propto f(\theta,x)=f(t_1)f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1)
\propto f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1).
$$
By inserting the likelihood and the priors we get the posterior distribution 
$$ 
f(\theta|x)\propto
\frac{1}{\beta^5}\lambda_0^{1+y_0}\lambda_1^{1+y_1}exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\right).
$$

## 3) 
To find the full conditionals for each of the elements in $\theta$ we omit all multiplicative factors in $f(\theta|x)$ that do not depend on the element. Realizing $y_0$ and $y_1$ are functions of $t_1$ we get the non normalised conditionals
$$
f(t_1|\lambda_0,\lambda_1,\beta,x)\propto \lambda_0^{1+y_0}\lambda_1^{1+y_1}\exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)) \propto \lambda_0^{1+y_0}\lambda_1^{1+y_1}\exp(-t_1(\lambda_0-\lambda_1)),
$$
$$
f(\lambda_0|t_1,\lambda_1,\beta,x)\propto \lambda_0^{1+y_0} \exp\left(-\lambda_0(1/\beta+t_1-t_0)\right),
$$
$$
f(\lambda_1|t_1,\lambda_0,\beta,x)\propto \lambda_1^{1+y_1} \exp\left(-\lambda_1(1/\beta+t_2-t_1)\right),
$$
$$
f(\beta|t_1,\lambda_0,\lambda_1,x)\propto \frac{1}{\beta^5}\exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)\right).
$$
From this we can see that $\lambda_i$ belongs to the gamma distribution with shape parameter $\alpha=2+y_i$ and rate parameter $\theta=1/\beta+t_{i+1}-t_i$ for $i=1,2$ and that $\beta$ comes from the inverse gamma distribution with shape $\alpha=4$ and rate $\theta=1+\lambda_0+\lambda_2$.

## 4)
We now implement a single site MCMC algorithm for $f(\theta|x)$. Since the full conditionals for $\lambda_0$, $\lambda_1$ and $\beta$ are known distributions we can just draw new values from these distributions. For $t_1$ we use a random walk proposal, namely the normal distribution with mean $t_1^{(i-1)}$ and standard deviation $\sigma$. Here $\sigma$ is a tuning parameter we can use to configure the Markov chain. Since this is a symmetric proposal the acceptance probability for the potential new value $\tilde t_1$ is then given by 
$$
\min\{1,\frac{f(\tilde t_1|\lambda_0,\lambda_1,\beta,x)}{f(t_1|\lambda_0,\lambda_1,\beta,x)}\} = \min\{1, \frac{\lambda_0^{1+\tilde y_0}\lambda_1^{1+\tilde y_1}}{\lambda_0^{1+y_0}\lambda_1^{1+y_1}}\exp((\tilde t_1-t_1)(\lambda_0-\lambda_1)) \}.
$$
where all parameters parameters not marked with "~" are taken from $\theta^{(i-1)}$ and $\tilde y_0$ and $\tilde y_1$ are the updated values given $\tilde t_1$. We also need to keep sure $t_0<t_1<t_2$. We enforce this by setting $\alpha=0$ when the proposed value $\tilde t_1$ does not satisfy this constraint. As initial values we use $t_1=(t_0+t_2)/2$ and draw the rest from $unif(0,1)$. 

```{r}
library(MASS)
library(invgamma)

logtarget <- function(x){ # x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  return (-5*log(x[4]) + (x[5]+1)*log(x[2]) + (189-x[5]+1)*log(x[3]) - (1+x[2]+x[3])/x[4] + x[1]*(x[3]-x[2]) + x[2]*t0 - x[3]*t2 )
}

fullcond <- function(x,r){ #x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  if (r==1){ #LOGSCALE!
    return( (x[5]+1)*log(x[2]) + (189-x[5]+1)*log(x[3]) + x[1]*(x[3]-x[2]) )
  }
  if (r==2){
    return (rgamma(1, shape=x[5]+2, rate=1/x[4]+x[1]-t0))
  }
  else if (r==3){
    return (rgamma(1, shape=189-x[5]+2, rate=1/x[4]+t2-x[1]))
  }
  else if (r==4){
    return (rinvgamma(1, shape=4, rate=1+x[2]+x[3]))
  }
}

mcmc_RW <- function(sd, ntimes){ #x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  x <- matrix(nrow=ntimes, ncol=5)
  # x[1,1] <- runif(1,t0,t2)
  x[1,1] <- (t2+t0)/2
  x[1,2:4] <- runif(3,0,1)
  x[1,5] <- sum(date<x[1,1])
  for(i in 2:ntimes){
    x[i,2] <- fullcond(x[i-1,],2)
    x[i,3] <- fullcond(x[i-1,],3)
    x[i,4] <- fullcond(x[i-1,],4)
    temp <- x[i-1,]
    t1 <- rnorm(1, temp[1], sd)
    temp[1] <- t1
    temp[5] <- sum(date<t1)
    alpha <- min(1,exp(fullcond(temp,1)-fullcond(x[i-1,],1)), na.rm=TRUE)
    if (runif(1)<alpha && t0<t1 && t1<t2){
      x[i,1] <- t1
      x[i,5] <- sum(date<=t1)
      }
    else{
      x[i,1] <- x[i-1,1]
      x[i,5] <- x[i-1,5]
    }
  }
  return(x)
}
```

## 5)
We set the tuning parameter $\sigma$ to 1, such that the proposal distribution for $t_1$ is the standard normal, and do 5000 steps of our algorithm.
```{r}
sd <- 1
ntimes <- 5000
x1 = mcmc_RW(sd, ntimes)

par(mfrow=c(2,2))
plot(x1[,1], type='l', xlab="i", ylab="t1")
plot(x1[,2], type='l', xlab="i", ylab="lambda0")
plot(x1[,3], type='l', xlab="i", ylab="lambda1")
plot(x1[,4], type='l', xlab="i", ylab="beta")

```





The trace plots above show the development of each parameter. The plots for $\lambda_i$ look exactly as we want, with a short burn-in period and with the looks of a wide band. For $\beta$ there is no visible burn-in, but there are some really large values, but this is expected with the rather large shape parameter in the inverse gamma distribution. Because these parameters are drawn directly from their full conditionals they also update in every iteration. This make them mix well the whole way. The proposed value for $t_1$ is not accepted at every step, such that this chain is much slower at exploring the whole domain. This means it may be smart to run the chain longer or change the tuning parameter $sigma$. Omitting the 1000 first steps seem to be a safe choice for the burn-in period for each of the four parameters.

To further evaluate the mixing properties we look at the autocorrelation function for each parameter.
```{r}
#Remove burn-in
burnin <- c(1:1000)

#Autocorrelations
par(mfrow=c(2,2))
acf(x1[-burnin,1], main="t1")
acf(x1[-burnin,2], main="lambda0")
acf(x1[-burnin,3], main="lambda1")
acf(x1[-burnin,4], main="beta")
```
We immediately see that $\lambda_0$, $\lambda_1$ and $\beta$ have close to correlation between each step and thus have very good mixing properties. This is probably a result of the new values being accepted at each step and that the new proposals are not very connected to the previous step. The autocorrelation for $t_1$ is not too bad, but we see that there is some correlation over a relatively large number of steps. This could be improved by increasing the standard devation in the proposal of $t1$.


```{r}
sd <- 1
ntimes <- 10000
x2 = mcmc_RW(sd, ntimes)

par(mfrow=c(2,2))
truehist(x2[-burnin,1], xlab="t1")
truehist(x2[-burnin,2], xlab="lambda0")
truehist(x2[-burnin,3], xlab="lambda1")
truehist(x2[-burnin,4], xlim=c(0,10), xlab="beta")



t1hat <- mean(x2[-burnin,1])
l0hat <- mean(x2[-burnin,2])
l1hat <- mean(x2[-burnin,3])
betahat <- mean(x2[-burnin,4])
c(t1hat, l0hat, l1hat, betahat)

par(mfrow=c(1,1))
plot(date, 1:189, xlab="Year", ylab="Cumulative number of disasters", sub="Cumulative number of coal-mining disasters in UK from 1851 to 1962")
lines(x=c(t0,t1hat), y=c(0,(t1hat-t0)*l0hat), col="red", lw=2)
lines(x=c(t1hat,t2), y=c((t1hat-t0)*l0hat, (t1hat-t0)*l0hat+(t2-t1hat)*l1hat), col="red", lw=2)
```

## 6) 
```{r}

```
## 7) 
Instead of single site updates we now do two block proposals. In the first block we keep $\beta$ unchanged and start by generating $\tilde t_1$ from a normal distribution with the current value of $t_1$ as mean and standard deviation $\sigma_1$. We then generate $\lambda_0$ and $\lambda_1$ from their joint full conditional given $\tilde t_1$, i.e. $f(\lambda_0, \lambda_1 | x, \tilde t_1, \beta)$. We observe that this joint density is the product of their marginal full conditionals and thus they are independent and can be generated from their marginals, given $\tilde t_1$. This block update is accepted with the usual Metropolis-Hastings acceptance probability, but with three new values instead of one.
The second block is identical to the first only switching the roles of $t_1$ and $\beta$ and using $\sigma_2$ as standard deviation in the proposal for $\beta$. We use the same initial values as before.

```{r}
mcmc_block <- function(sd, ntimes)
{
  x <- matrix(nrow=ntimes, ncol=5)
  x[1,1] <- (t0+t2)/2
  x[1,2:4] <- runif(3,0,1)
  x[1,5] <- sum(date<x[1,1])
  for(i in 2:ntimes)
  {
    t1 <- rnorm(1,mean=x[i-1,1],sd=sd[1])
    if (t0<t1 && t1<t2){
      y0 <- sum(date<t1)
      temp <- x[i-1,]
      temp[1] <- t1
      temp[5] <- y0
      lam0 <- fullcond(temp,2)
      lam1 <- fullcond(temp,3)
      temp[2] <- lam0
      temp[3] <- lam1
      alpha <- min(1,exp(logtarget(temp)-logtarget(x[i-1,])), na.rm=TRUE)
      if (runif(1)<alpha){
        x[i,] <- temp
      }
      else{
        x[i,] <- x[i-1,]
      }
    }
    else{
      x[i,] <- x[i-1,]
    }
    
    temp <- x[i,]
    beta <- rnorm(1,mean=temp[4], sd=sd[2])
    if (beta>0){
      temp[4] <- beta
      lam0 <- fullcond(temp,2)
      lam1 <- fullcond(temp,3)
      temp[2] <- lam0
      temp[3] <- lam1
      alpha <- min(1,exp(logtarget(temp)-logtarget(x[i-1,])),na.rm=TRUE)
      if (runif(1) < alpha){
        x[i,] <- temp
      }
    }
  }
  return(x)
}
```

## 8)
In this block Metropolis-Hastings algorithm we have two tuning parameters, $\sigma=(\sigma_1,\sigma_2)$. ...
```{r}
sd.b <- c(2,2)
ntimes <- 10000

x.b <- mcmc_block(sd.b,ntimes)

par(mfrow=c(2,2))
plot(x.b[,1], type='l', xlab="i", ylab="t1")
plot(x.b[,2], type='l', xlab="i", ylab="lambda0")
plot(x.b[,3], type='l', xlab="i", ylab="lambda1")
plot(x.b[,4], type='l', xlab="i", ylab="beta")

burnin <- c(1:1000)

par(mfrow=c(2,2))
truehist(x.b[-burnin,1], xlab="t1")
truehist(x.b[-burnin,2], xlab="lambda0")
truehist(x.b[-burnin,3], xlab="lambda1")
truehist(x.b[-burnin,4], xlim=c(0,10), xlab="beta")

par(mfrow=c(2,2))
acf(x.b[-burnin,1])
acf(x.b[-burnin,2])
acf(x.b[-burnin,3])
acf(x.b[-burnin,4])

mean(x.b[-burnin,1])
mean(x.b[-burnin,2])
mean(x.b[-burnin,3])
mean(x.b[-burnin,4])

cov(x.b[-burnin,2], x.b[-burnin,3])
```

# Problem B: INLA for Gaussian Data
In this problem we are going to use the simulated data in the Gaussiandata.txt file. We start by plotting the dataset. 
```{r}
data=read.table("https://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/exercise2/Gaussiandata.txt")
names(data)="y"
plot(data$y)
```

## 1)
A latent Gaussian model, LGM, consists of three elements: a likelihood model, a latent Gaussian field and a vector of hyperparameters. In our model the observations $y_t$ are assumed independent and Gaussian distributed with mean $\eta_t$ and known unit varuance. Hence, the likelihood model is given as $y_t|\eta_t=\mathcal{N}(\eta_t,1); t=1,...T$. The linear predictor $\eta_t$ is linked to a smooth effect of time $t$ as $\eta_t=f_t$. For the vector $\mathbf{f}=(f_1,\dots, f_T)$ we have the second order random walk as the prior distribution, such that
$$
\pi(\mathbf{f}|\theta) \propto \theta^{(T-2)/2}exp\left\{\frac{\theta}{2}\sum_{t=3}^T[f_t-2f_{t-1}+f_{t-2}]^2 \right\} = \mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}).
$$
Hence, the latent field $\mathbf{f}$ is a Gaussian Markov random field, GMRF, with sparce precision matrix $\mathbf{Q}(\theta)^{-1}$. The precision parameter $\theta$ controls the smoothness of the vector $\mathbf{f}$ and is our hyperparameter, $\theta \sim \textrm{gamma}(1,1)$.

It is possible to use INLA to estimate the parameters because our inferential interest lies in the posterior marginal for the smooth effect $\pi(\eta_t|\mathbf{y})$,$t=1,\dots,T$, and the LGM fulfill the following assumtions. Each data point $y_t$ depends only on one of the elements in the latent Gausian field $\mathbf{f}$, the linear predictor $\eta_t$. The hyperparameter vector should be small, and in our case it contains only one parameter $\theta$. The precion matrix $\mathbf{Q}(\theta)^{-1}$ is sparse, and the linear predictor depends linearly on the unkown smooth function of temporal effects.  

## 2) 
In this exercise we implement a block Gibbs sampling algorithm for $\pi(\eta,\theta|y)$, where we update the parameters $\theta$ and $\eta$ iteratively by sampling from the full conditional distributions $\pi(\theta|\eta,y)$ and $\pi(\eta|\theta,y)$. 


$$
\pi(\eta,\theta|y)=\frac{\pi(\theta,\eta,y)}{\pi(y)} \propto \pi(y|\eta)\pi(\eta|\theta)\pi(\theta) \propto \theta^{(T-2)/2}\exp\left\{ -\theta -\frac{1}{2}\left(\eta^T(Q+I)\eta + y^Ty \right) +y^T\eta  \right\}
$$

```{r}
library(MASS)
library(Matrix)
library(dae)
library(matrixStats)

A_matrix <- function(T){
  Q <- diag(1,T,T-2)
  Q[row(Q) - col(Q) == 1] <- -2
  Q[row(Q) - col(Q) == 2] <- 1
  QQ=Q%*%t(Q)
  return(QQ)
}

block_Gibbs <- function(y, nsamples) {
  theta <- rep(0,nsamples)
  eta <- matrix(0,nsamples,T)
  #initial values
  theta[1] <- rgamma(1,1,1)  
  eta[1,] <- y 
  for (i in 2:nsamples){ 
    ## update theta
    rate=1+0.5*t(eta[i-1,])%*%A%*%eta[i-1,]
    theta[i] <- rgamma(1,shape=T/2,rate=rate)
    ## update the vector eta with the new theta
    I=diag(1,T)
    var=solve(A*theta[i]+I,sparse=TRUE)
    mean=var%*%y
    eta[i,] <- rmvnorm(mean, var)
      
  }
  list=cbind(theta=theta, eta=eta) 
  return(list)
}

n=10000
y=data$y
T <- length(y)
A <- A_matrix(T)
f_gibbs=block_Gibbs(y,n)
t=seq(1,20,1)

m <- colMeans(f_gibbs[-c(1:1000),])
v <- colSds(f_gibbs[-c(1:1000),])
m_upper <- m+qnorm(0.025)*v
m_lower <- m-qnorm(0.025)*v

plot(t,y)
lines(t,m[-1])
lines(t,m_upper[-1],col="red")
lines(t,m_lower[-1],col="red")

truehist(f_gibbs[,1])
truehist(f_gibbs[-c(1:1000),11])

```

## 3) 
```{r}
pi_theta <- function(theta){
  A<-A_matrix(T)
  I=diag(1,T)
  det <- rep(0,length(theta))
  pi <- rep(0,length(theta))
  for (i in 1:length(theta)){
    det<-det(A*theta[i]+I)
    pi[i] <- theta[i]^(T/2-1)*exp(-theta[i])*exp(-0.5*t(y)%*%(I-solve(A*theta[i]+I))%*%y)/det^0.5
  }
  return (pi)
} 

T=20
mode = optimise(pi_theta,lower = 0, upper = 6, maximum = T)$maximum
delta_k=0.01
theta <- seq(0,9,delta_k)
const = integrate(pi_theta, lower = 0, upper = 10)$value
pi_theta_k <- pi_theta(theta)/const
#s <- smooth.spline(theta,pi_theta_k/const)
#plot(theta,pi_theta(theta)/const)
#lines(s)

```

## 4) 
```{r}
eta <- seq(-1,1,0.01)
set.seed(0)
pi_eta_y_theta <- function(theta,eta,i=10){
  QI <- A_matrix(T)*theta+diag(1,T)
  QI_i <- QI[i,i]
  y_i <- y[i]
  mean <- y_i/QI_i
  var <- 1/QI_i
  pi <- dnorm(eta,mean, sqrt(var))
  return(pi)
}

pi_eta_y <- function(theta){
  sum=rep(0,length(eta))
  for (j in 1:length(theta)){
    sum = sum + (pi_eta_y_theta(theta[j],eta)*pi_theta_k[j]*delta_k)
  }
  return (sum)
}

eta_10 <- pi_eta_y(theta)
plot(eta,eta_10)
```


## 5) 
```{r}
#install.packages("INLA", repos=c(getOption("repos"),
#INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
library("INLA")
x=(1:20)
df=data.frame(y=data$y,x=x)

# specify the prior
my.hyper <- list(theta = list(prior="log.gamma", param=c(1,1)))
# specify the linear predictor
formula <- y ~ -1 + f(x, model = "rw2", hyper = my.hyper, constr=FALSE)
#Fit the model
result <- inla(formula=formula, family="gaussian", data=df,verbose = FALSE, control.family = list(hyper = list(prec=list(initial=0,fixed=TRUE)))) 

plot(result$summary.random$x$mean)
plot(result$marginals.hyperpar$`Precision for x`,xlim=c(0,6))
plot(result$marginals.random$x$index.10, xlim=c(-2,2))
summary(result)
```

