---
title: "TMA4300 - Exercise 2"
author: "Camilla og Vebj√∏rn"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A: The coal-mining disaster data
In this problem we are going to analyse a data set of time intervals between successive coal-mining disasters in the UK involving ten or more men killed in the period March 15th 1851 to March 22nd 1962. 

## 1) 
We start by making a plot with the cumulative number of disasters along the y-axis and year along the x-axis. 
```{r fig.height=4, fig.width=6 }
library(boot)
date <- coal$date #fetch data
t0 <- date[1] #start date
t2 <- date[191] #end date
date <- date[-c(1,191)] #data
plot(date, 1:189, xlab="year", ylab="cumulative number of disasters",
     sub="Cumulative number of coal-mining disasters in UK from 1851 to 1962")
```

From the plot we can see that the cumulative number of disasters increases almost linearly until around 1900 and from there the trend is close to linear, but with a less steep slope than before. In the latter period there are some longer periods without any disaster, but also some with very frequent disasters. The period after 1945 seem to be the best in the period of these data, which is natural due to the development of technology, and it could maybe also be linked to the end of world war 2.

## 2)
We want to model these data as a nonhomogeneous Poisson process with constant rate $\lambda_k$ in the period $t_k$ to $t_{k+1}$, $k=0,\dots,n$. Dividing the data into two periods, with the split at $t_1\approx1900$, seems reasonable.
We then get the likelihood
$$
f(x|t_1,\lambda_0,\lambda_1)=\lambda_0^{y_0}\lambda_1^{y_1}exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)),
$$
where $x$ is the observed data and $y_k$ is the number of observed disasters in the period $t_k$ to $t_{k+1}$. The priors $f(t_1)\sim \textrm{unif}(t_0,t_2)$ and $f(\lambda_i|\beta) \sim \textrm{gamma}(\alpha=2,\beta)$, for $\lambda_i=1,2$, are independent. For the hyper parameter $\beta$ we use the improper prior $f(\beta)\propto exp(-1/\beta) / \beta$.

Hence, the posterior distribution for $\theta=(t_1,\lambda_0,\lambda_1,\beta)$ given $x$ is, up to a normalising constant, given by
$$
f(\theta|x)=\frac{f(\theta,x)}{f(x)}\propto f(\theta,x)=f(t_1)f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1)
\propto f(\beta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(x|t_1,\lambda_0,\lambda_1).
$$
By inserting the likelihood and the priors we get the posterior distribution 
$$ 
f(\theta|x)\propto
\frac{1}{\beta^5}\lambda_0^{1+y_0}\lambda_1^{1+y_1}exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\right).
$$

## 3) 
To find the full conditionals for each of the parameters in $\theta$ we omit all multiplicative factors in $f(\theta|x)$ that do not depend on the parameters. Realizing $y_0$ and $y_1$ are functions of $t_1$ we get the non normalised conditionals
$$
f(t_1|\lambda_0,\lambda_1,\beta,x)\propto \lambda_0^{1+y_0}\lambda_1^{1+y_1}\exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)) \propto \lambda_0^{1+y_0}\lambda_1^{1+y_1}\exp(-t_1(\lambda_0-\lambda_1)),
$$
$$
f(\lambda_0|t_1,\lambda_1,\beta,x)\propto \lambda_0^{1+y_0} \exp\left(-\lambda_0(1/\beta+t_1-t_0)\right),
$$
$$
f(\lambda_1|t_1,\lambda_0,\beta,x)\propto \lambda_1^{1+y_1} \exp\left(-\lambda_1(1/\beta+t_2-t_1)\right),
$$
$$
f(\beta|t_1,\lambda_0,\lambda_1,x)\propto \frac{1}{\beta^5}\exp\left(-\frac{1}{\beta}(1+\lambda_0+\lambda_1)\right).
$$
From this we can see that $\lambda_i$ belongs to the gamma distribution with shape parameter $\alpha=2+y_i$ and rate parameter $\theta=1/\beta+t_{i+1}-t_i$ for $i=0,1$ and that $\beta$ comes from the inverse gamma distribution with shape $\alpha=4$ and rate $\theta=1+\lambda_0+\lambda_2$.

## 4)
We now implement a single site MCMC algorithm for $f(\theta|x)$. Since the full conditionals for $\lambda_0$, $\lambda_1$ and $\beta$ are known distributions we can just draw new values from these distributions. For $t_1$ we use a random walk proposal, namely the normal distribution with the current value of $t_1$ as mean and standard deviation $\sigma$. Here $\sigma$ is a tuning parameter we can use to configure the Markov chain. Since this is a symmetric proposal the acceptance probability for the potential new value $\tilde t_1$ is then given by 
$$
\min\{1,\frac{f(\tilde t_1|\lambda_0,\lambda_1,\beta,x)}{f(t_1|\lambda_0,\lambda_1,\beta,x)}\} = \min\{1, \frac{\lambda_0^{1+\tilde y_0}\lambda_1^{1+\tilde y_1}}{\lambda_0^{1+y_0}\lambda_1^{1+y_1}}\exp((\tilde t_1-t_1)(\lambda_0-\lambda_1)) \}.
$$
where all parameters parameters not marked with "~" are taken from the former step in the chain and $\tilde y_0$, $\tilde y_1$ are the updated values given $\tilde t_1$. We also need to keep sure $t_0<t_1<t_2$. We enforce this by setting $\alpha=0$ when the proposed value $\tilde t_1$ does not satisfy this constraint. As initial values we use $t_1=(t_0+t_2)/2$ and draw the rest from $unif(0,1)$. 

```{r}
library(MASS) #histograms
library(invgamma) #inverse gamma distribution

#Target distribution on log-scale
logtarget <- function(x){ # x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  return (-5*log(x[4]) + (x[5]+1)*log(x[2]) + (189-x[5]+1)*log(x[3]) 
          - (1+x[2]+x[3])/x[4] + x[1]*(x[3]-x[2]) + x[2]*t0 - x[3]*t2 )
}

#Full conditionals
#r=1 gives the full conditional distribution of t1 on logscale
#r=2,3,4 draws a new value from the full conditional distributions of lam0,lam1,beta
fullcond <- function(x,r){ #x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  if (r==1){ #logscale
    return( (x[5]+1)*log(x[2]) + (189-x[5]+1)*log(x[3]) + x[1]*(x[3]-x[2]) )
  }
  if (r==2){
    return (rgamma(1, shape=x[5]+2, rate=1/x[4]+x[1]-t0))
  }
  else if (r==3){
    return (rgamma(1, shape=189-x[5]+2, rate=1/x[4]+t2-x[1]))
  }
  else if (r==4){
    return (rinvgamma(1, shape=4, rate=1+x[2]+x[3]))
  }
}

#single site MCMC algorithm
mcmc_RW <- function(sd, ntimes){ #x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  #Initial values
  x <- matrix(nrow=ntimes, ncol=5)
  x[1,1] <- (t2+t0)/2
  x[1,2:4] <- runif(3,0,1)
  x[1,5] <- sum(date<x[1,1])
  for(i in 2:ntimes){
    #Gibbs sampling for lam0,lam1,beta
    x[i,2] <- fullcond(x[i-1,],2)
    x[i,3] <- fullcond(x[i-1,],3)
    x[i,4] <- fullcond(x[i-1,],4)
    #rw proposal for t1
    temp <- x[i-1,]
    t1 <- rnorm(1, temp[1], sd)
    temp[1] <- t1
    temp[5] <- sum(date<t1)
    alpha <- min(1,exp(fullcond(temp,1)-fullcond(x[i-1,],1)), na.rm=TRUE)
    #Accept only if t0<t1<t2
    if (runif(1)<alpha && t0<t1 && t1<t2){
      x[i,1] <- t1
      x[i,5] <- sum(date<=t1)
      }
    else{
      x[i,1] <- x[i-1,1]
      x[i,5] <- x[i-1,5]
    }
  }
  return(x)
}
```

## 5)
We set the tuning parameter $\sigma$ to 1, such that the proposal distribution for $t_1$ is the normal distribution with standard deviation $\sigma$ and mean the previous value of $t_1$. At this point we do only 2000 steps of our algorithm to easier evaluate its properties in the early steps.
```{r}
#tuning parameter
sd <- 1
#number of steps in MC
ntimes <- 2000
#run algorithm
x1 = mcmc_RW(sd, ntimes)
#trace plots
par(mfrow=c(2,2))
plot(x1[,1], type='l', xlab="i", ylab="t1")
plot(x1[,2], type='l', xlab="i", ylab="lambda0")
plot(x1[,3], type='l', xlab="i", ylab="lambda1")
plot(x1[,4], type='l', xlab="i", ylab="beta")
```





The trace plots above show the development of each parameter. The plots for $\lambda_i$ look exactly as we want, with a short burn-in period and with the looks of a wide band. For $\beta$ there is no visible burn-in, but there are some really large values. However, this is expected with the rather large shape parameter in the inverse gamma distribution, which makes the distribution have a very long tail. Because these parameters are drawn directly from their full conditionals they also update in every iteration. This make them mix well the whole way. The proposed value for $t_1$ is not accepted at every step, such that this chain is much slower at exploring the whole domain. This means it may be smart to run the chain longer or change the tuning parameter $\sigma$. Judging by the plots of especially $t_1$ over several runs, the burn-in period seems vary between 50 to 500 steps. In order to have a buffer we choose to omit the 1000 first steps for each of the four parameters in the later analysis.

To further evaluate the mixing properties we look at the autocorrelation function for each parameter.
```{r}
#Define burn-in period
burnin <- c(1:1000)

#Autocorrelations
par(mfrow=c(2,2))
acf(x1[-burnin,1], main="t1")
acf(x1[-burnin,2], main="lambda0")
acf(x1[-burnin,3], main="lambda1")
acf(x1[-burnin,4], main="beta")
```
We immediately see that $\lambda_0$, $\lambda_1$ and $\beta$ have close to zero correlation between each step and thus have very good mixing properties. This is probably a result of the new values being accepted at each step and that the new proposals are not very connected to the previous step. The autocorrelation for $t_1$ is not too bad, but we see that there is some correlation over a relatively large number of steps. This could be improved by increasing the standard devation in the proposal of $t1$.

To get good estimates of the marginal posterior distributions we run the algorithm with 50000 steps and ommit the first 1000 steps from the estimations.
```{r}
#Run new chain
sd <- 1
ntimes <- 50000
x2 = mcmc_RW(sd, ntimes)

#Estimated means
t1hat <- mean(x2[-burnin,1])
l0hat <- mean(x2[-burnin,2])
l1hat <- mean(x2[-burnin,3])
betahat <- mean(x2[-burnin,4])
c(t1hat, l0hat, l1hat, betahat)
```

In order to evaluate the simulated values we recall that the number of disasters is modelled as a Poisson process. This means that the cumulative number of disasters in each period is a linear function of time, with slopes $\lambda_0$ and $\lambda_1$. Hence, we can use the estimated means of $t_1$, $\lambda_0$ and $\lambda_1$ to plot the estimated cumulative number of events together with the data.
```{r}
#Plot the estimated means together with data
par(mfrow=c(1,1))
plot(date, 1:189, xlab="Year", ylab="Cumulative number of disasters", 
     sub="Cumulative number of coal-mining disasters in UK from 1851 to 1962")
lines(x=c(t0,t1hat), y=c(0,(t1hat-t0)*l0hat), col="red", lw=2)
lines(x=c(t1hat,t2), y=c((t1hat-t0)*l0hat, (t1hat-t0)*l0hat+(t2-t1hat)*l1hat), 
      col="red", lw=2)
```
In the figure the points display the real data and the red lines show the posterior estimates of the process.  The plot shows that the estimated means represent the data very well. It seems like a nonhomogeneous Poisson process with two different intensities is a proper choice of model for these data.

It is difficult to directly evaluate the simulated value of $\beta$. However, the only interest we really have in $\beta$ is the role it plays in generating the $\lambda_i$. Since these parameter values work nicely, the $\beta$ works too.

## 6)
Now we run 50000 steps of the algorithm with $\sigma\in\{0.2,5,20\}$. And display trace plots and autocorrelation for $t_1$ for each case. We only display $t_1$ because it has proven to be the parameter that is most difficult to handle for the algorithm.
```{r}
sd1 <- .2
sd2 <- 5
sd3 <- 20
ntimes <- 50000
x3 <- mcmc_RW(sd1,ntimes)
x4 <- mcmc_RW(sd2,ntimes)
x5 <- mcmc_RW(sd3,ntimes)
```

```{r}
par(mfrow=c(1,2))
plot(x3[1:10000,1], type='l', xlab="i", ylab="t1")
acf(x3[-burnin,1], main="t1")
```
The first chain was generated with $\sigma=0.2$. The trace plot shows us that the value for $t_1$ do very short steps and therefore the mixing is very slow. This also leads to a rather long burn-in period and we cannot say for sure that the burn-in period is over even after several thousand steps. The autocorrelation function shows the same story, that points are highly correlated over long distances. Thus, the chain converges very slowly.
```{r}
par(mfrow=c(1,2))
plot(x4[1:2000,1], type='l', xlab="i", ylab="t1")
acf(x4[-burnin,1], main="t1")
```
For $\sigma=5$ everything looks a lot better, even when we plot only the first 2000 steps. The burn-in period seem to be just around 10 steps and the autocorrelation is even better here than with $\sigma=1$.
```{r}
par(mfrow=c(1,2))
plot(x5[1:2000,1], type='l', xlab="i", ylab="t1")
acf(x5[-burnin,1], main="t1")
```
One could think that increasing $\sigma$ even more would only be positive, but if we look at the situation with $\sigma=20$ we see that the trace plot is not fluctuating as much as we want. Most steps seem to be long, but a lot of steps are rejected. This is because many proposed steps are too long and move far away from the optimal value. We also see that the autocorrelation is worse than for $\sigma=5$. Hence from the four different values for $\sigma$ we tried, $\sigma=5$ is the best one. Note that there are probably other values that are even better, but $\sigma=5$ is clearly sufficiently good to use.

Now we plot histograms approximating the posterior marginal distribution of $\lambda_0$ from each of the four chains, after omitting the 10000 first steps due to the long burn-in with $\sigma=0.2$.
```{r}
burnin=c(1:10000)
par(mfrow=c(2,2))
truehist(x3[-burnin,2], xlab="marginal for lambda0 with sigma=0.2")
truehist(x2[-burnin,2], xlab="marginal for lambda0 with sigma=1")
truehist(x4[-burnin,2], xlab="marginal for lambda0 with sigma=5")
truehist(x5[-burnin,2], xlab="marginal for lambda0 with sigma=20")
```
We see that all histograms are close to identical and it seems safe to assume that they have the same limiting distribution.

## 7) 
Instead of single site updates we now want to do two block proposals. In the first block we keep $\beta$ unchanged and start by generating $\tilde t_1$ from a normal distribution with the current value of $t_1$ as mean and standard deviation $\sigma_1$. We then generate $\lambda_0$ and $\lambda_1$ from their joint full conditional given $\tilde t_1$, that is
$$
f(\lambda_0, \lambda_1 | x, \tilde t_1, \beta) \propto \lambda_0^{\tilde y_0+1}\lambda_1^{\tilde y_1+1}\exp\{-\lambda_0(1/\beta+\tilde t_1-t_0)-\lambda_1(1/\beta+t_2-\tilde t_1)\}.
$$
We observe that this is simply the product of the marginal full conditionals of $\lambda_0$ and $\lambda_1$. Thus they are independent and can be generated from their separate marginals $f(\lambda_i | x, \tilde t_1, \beta)$. Since we now update three values at the same time it is reasonable to just use the joint density function of all the parameters in the acceptance probability. Thus, 
$$
\alpha = \min\{1,\frac{f(\tilde\theta|x)}{f(\theta|x)}\},
$$
where $\tilde\theta=(\tilde t_1, \tilde\lambda_0, \tilde\lambda_1, \beta)$.
The second block is equivalent to the first, but we are now keeping $t_1$ constant and instead proposing $\beta$ from the normal distribution with $\sigma_2$ as standard deviation. We use the same initial values as in the single site algorithm.

```{r}
#block MCMC algorithm
mcmc_block <- function(sd, ntimes){ #x1=t1, x2=lam0, x3=lam1, x4=beta, x5=y0
  #Initial values
  x <- matrix(nrow=ntimes, ncol=5)
  x[1,1] <- (t0+t2)/2
  x[1,2:4] <- runif(3,0,1)
  x[1,5] <- sum(date<x[1,1])
  for(i in 2:ntimes)
  {
    #Block 1
    #rw proposal
    t1 <- rnorm(1,mean=x[i-1,1],sd=sd[1])
    #reject if not t0<t1<t2
    if (t0<t1 && t1<t2){
      #update y0(t1)
      y0 <- sum(date<t1)
      temp <- x[i-1,]
      temp[1] <- t1
      temp[5] <- y0
      #draw lambdas from full conditionals given new t1
      lam0 <- fullcond(temp,2)
      lam1 <- fullcond(temp,3)
      temp[2] <- lam0
      temp[3] <- lam1
      #acceptance probabilty
      alpha <- min(1,exp(logtarget(temp)-logtarget(x[i-1,])), na.rm=TRUE)
      if (runif(1)<alpha){
        x[i,] <- temp
      }
      else{
        x[i,] <- x[i-1,]
      }
    }
    else{
      x[i,] <- x[i-1,]
    }
    #Block 2
    #rw proposal
    beta <- rnorm(1,mean=x[i-1,4], sd=sd[2])
    #reject if beta<=0
    if (beta>0){
      temp <- x[i,]
      temp[4] <- beta
      #draw lambdas from full conditionals given new beta
      lam0 <- fullcond(temp,2)
      lam1 <- fullcond(temp,3)
      temp[2] <- lam0
      temp[3] <- lam1
      #acceptance probability
      alpha <- min(1,exp(logtarget(temp)-logtarget(x[i-1,])),na.rm=TRUE)
      if (runif(1) < alpha){
        x[i,] <- temp
      }
    }
  }
  return(x)
}
```

## 8)
In this block Metropolis-Hastings algorithm we have two tuning parameters, $\sigma=(\sigma_1,\sigma_2)$. We now test the algorithm with multiple different combinations of $\sigma$ and look at trace plots and autocorrelation.
```{r}
#Define different sigmas to test
sd.b1 <- c(1,1)
sd.b2 <- c(.2,1)
sd.b3 <- c(5,1)
sd.b4 <- c(20,1)
sd.b5 <- c(1,.2)
sd.b6 <- c(1,5)
sd.b7 <- c(1,20)
sd.b8 <- c(5,5)
ntimes <- 2000

#Run algorithm for each sigma
x.b1 <- mcmc_block(sd.b1,ntimes)
x.b2 <- mcmc_block(sd.b2,ntimes)
x.b3 <- mcmc_block(sd.b3,ntimes)
x.b4 <- mcmc_block(sd.b4,ntimes)
x.b5 <- mcmc_block(sd.b5,ntimes)
x.b6 <- mcmc_block(sd.b6,ntimes)
x.b7 <- mcmc_block(sd.b7,ntimes)
x.b8 <- mcmc_block(sd.b8,ntimes)

#Trace plots
par(mfrow=c(2,2))
plot(x.b1[,1], type='l', xlab="sigma=(1,1)", ylab="t1")
plot(x.b2[,1], type='l', xlab="sigma=(0.2,1)", ylab="t1")
plot(x.b3[,1], type='l', xlab="sigma=(5,1)", ylab="t1")
plot(x.b4[,1], type='l', xlab="sigma=(20,1)", ylab="t1")
par(mfrow=c(2,2))
plot(x.b5[,1], type='l', xlab="sigma=(1,.2)", ylab="t1")
plot(x.b6[,1], type='l', xlab="sigma=(1,5)", ylab="t1")
plot(x.b7[,1], type='l', xlab="sigma=(1,20)", ylab="t1")
plot(x.b8[,1], type='l', xlab="sigma=(5,5)", ylab="t1")

#Define burn-in
burnin = c(1:1000)

#Autocorrelation
par(mfrow=c(2,2))
acf(x.b1[-burnin,1],main="sigma=(1,1)")
acf(x.b2[-burnin,1],main="sigma=(0.2,1)")
acf(x.b3[-burnin,1],main="sigma=(5,1)")
acf(x.b4[-burnin,1],main="sigma=(20,1)")
par(mfrow=c(2,2))
acf(x.b5[-burnin,1],main="sigma=(1,0.2)")
acf(x.b6[-burnin,1],main="sigma=(1,5)")
acf(x.b7[-burnin,1],main="sigma=(1,20)")
acf(x.b8[-burnin,1],main="sigma=(5,5)")
```

We observe very similar patterns as with the single site update. Having $\sigma_i=0.2$ is too small for both parameters and lead to very correlated steps and slow mixing. Having $\sigma_i=20$ works better, but also here too many steps get rejected. Out of the tested combinations of tuning parameters we get the best results with $\sigma=(5,1)$, where the burn-in is very short and the autocorrelation goes to zero before lag 10. This makes sense since the scale of $t_1$ is much larger than the scale of $\beta$ and using $\sigma_2=5$ in the proposal for $\beta$ is too large.

It seems like it is easier to make each step less correlated and hence get better mixing with the block update. The burn-in period varies between each time the algorithm is run, but it seems like it is easier to handle bad tuning parameters if you use the block update, especially if at least one of the tuning parameters is reasonably chosen.

Since $\sigma=(5,1)$ gave the best results we keep using that in the rest of the exercise. Since we have mostly compared the behaviour of $t_1$ we confirm our decision by displaying trace plots for all parameters in the winning Markov chain.
```{r}
par(mfrow=c(2,2))
plot(x.b8[,1], type='l', xlab="i", ylab="t1")
plot(x.b8[,2], type='l', xlab="i", ylab="lambda0")
plot(x.b8[,3], type='l', xlab="i", ylab="lambda1")
plot(x.b8[,4], type='l', xlab="i", ylab="beta")
```
Here we see that everything is fine and that omitting the 500 first steps should be more than enough when making estimates. In order to get good estimates we run the chain for 50000 steps and plot histograms of each parameter to estimate the marginal posterior distributions.

```{r}
burnin <- c(1:500)
ntimes <- 50000
#Run chain longer
x.b8 <- mcmc_block(sd.b8, ntimes)

#Make histograms
par(mfrow=c(2,2))
truehist(x.b8[-burnin,1], xlab="t1")
truehist(x.b8[-burnin,2], xlab="lambda0")
truehist(x.b8[-burnin,3], xlab="lambda1")
truehist(x.b8[-burnin,4], xlim=c(0,10), xlab="beta")
```
The histograms show that the marginal posteriors for $\lambda_0$ and $\lambda_1$ are close to a Gaussian distribution, but a little bit skewed, which fits nicely to a gamma-distribution. The histogram for $\beta$ has a long tail, which fits well to the inverse-gamma distribution. Thus, it seems like the three parameters generated directly from known full conditionals have the same kind of posterior distribution. The parameter $t_1$ does not have a nice and regular posterior distribution. This is not very surprising given the nature of the problem, where many values for $t_1$ could be considered.

We can estimate the posterior means from the sample and verify them by plotting against the real data as earlier.
```{r}
#Estimate means
t1hat2 <- mean(x.b8[-burnin,1])
l0hat2 <- mean(x.b8[-burnin,2])
l1hat2 <- mean(x.b8[-burnin,3])
betahat2 <- mean(x.b8[-burnin,4])
c(t1hat2, l0hat2, l1hat2, betahat2)

#Plot estimates against real data
par(mfrow=c(1,1))
plot(date, 1:189, xlab="Year", ylab="Cumulative number of disasters", 
     sub="Cumulative number of coal-mining disasters in UK from 1851 to 1962")
lines(x=c(t0,t1hat2), y=c(0,(t1hat2-t0)*l0hat2), col="red", lw=2)
lines(x=c(t1hat2,t2), y=c((t1hat2-t0)*l0hat2, (t1hat2-t0)*l0hat2+(t2-t1hat2)*l1hat2), 
      col="red", lw=2)
```
From the print-out we quickly see that these values are the almost the same as those from the single site algorithm, and the two corresponding plots are naturally almost indistinguishable.

We also estimate $Cov[\lambda_0,\lambda_1|x]$.
```{r}
#Cov[lambda0,lambda1]:
cov(x.b8[-burnin,2], x.b8[-burnin,3])
```
The print-out shows that the covariance between $\lambda_0$ and $\lambda_1$ is practically zero. This was expected since their joint full conditional is the product of the two marginal full conditionals which means they are independent.

# Problem B: INLA for Gaussian Data
In this problem we are going to use the simulated data in the Gaussiandata.txt file. We start by plotting the dataset. 
```{r}
data=read.table(
  "https://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/exercise2/Gaussiandata.txt")
names(data)="y"
plot(data$y)
```

## 1)
A latent Gaussian model, LGM, consists of three elements: a likelihood model, a latent Gaussian field and a vector of hyperparameters. In our model the observations $y_t$ are assumed independent and Gaussian distributed with mean $\eta_t$ and known unit varuance. Hence, the likelihood model is given as $y_t|\eta_t=\mathcal{N}(\eta_t,1); t=1,...T$. The linear predictor $\eta_t$ is linked to a smooth effect of time $t$ as $\eta_t=f_t$. For the vector $\mathbf{f}=(f_1,\dots, f_T)$ we have the second order random walk as the prior distribution, such that
$$
\pi(\mathbf{f}|\theta) \propto \theta^{(T-2)/2}exp\left\{\frac{\theta}{2}\sum_{t=3}^T[f_t-2f_{t-1}+f_{t-2}]^2 \right\} = \mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}).
$$
Hence, the latent field $\mathbf{f}$ is a Gaussian Markov random field, GMRF, with sparce precision matrix $\mathbf{Q}(\theta)$. The precision parameter $\theta$ controls the smoothness of the vector $\mathbf{f}$ and is our hyperparameter, $\theta \sim \textrm{gamma}(1,1)$.

It is possible to use INLA to estimate the parameters because our inferential interest lies in the posterior marginal for the smooth effect $\pi(\eta_t|\mathbf{y})$,$t=1,\dots,T$, and the LGM fulfill the following assumtions. Each data point $y_t$ depends only on one of the elements in the latent Gausian field $\mathbf{f}$, the linear predictor $\eta_t$. The hyperparameter vector should be small, and in our case it contains only one parameter $\theta$. The precision matrix $\mathbf{Q}(\theta)$ is sparse, and the linear predictor depends linearly on the unkown smooth function of temporal effects.  

## 2) 
In this exercise we implement a block Gibbs sampling algorithm for $\pi(\boldsymbol{\eta},\theta|\mathbf{y})$, where we update the parameters $\theta$ and $\eta$ iteratively by sampling from the full conditional distributions $\pi(\theta|\boldsymbol{\eta},\mathbf{y})$ and $\pi(\boldsymbol{\eta}|\theta,\mathbf{y})$. We start by finding the posterior distribution for $\theta$ and $\boldsymbol{\eta}$ given $\mathbf{y}$,


$$
\pi(\boldsymbol{\eta},\theta|\mathbf{y})=\frac{\pi(\theta,\boldsymbol{\eta},\mathbf{y})}{\pi(\mathbf{y})} \propto \pi(\mathbf{y}|\boldsymbol{\eta})\pi(\boldsymbol{\eta}|\theta)\pi(\theta) \propto \theta^{(T-2)/2}\exp\left\{ -\theta -\frac{1}{2}\left(\boldsymbol{\eta}^T(\mathbf{Q}+\mathbf{I})\boldsymbol{\eta}+ \mathbf{y}^T\mathbf{y} \right) +\mathbf{y}^T\boldsymbol{\eta} \right\},
$$
where $\mathbf{I}$ is the identity matrix with dimension $T \times T$. $\mathbf{Q}$ is the precision matrix, and can be written as $\frac{1}{\theta}\mathbf{L}\mathbf{L}^T$, where 
$$
\mathbf{L}=\begin{bmatrix}
1 &  &  &  &   & \\
-2 & \ddots &  &  &  &  \\
1 & \ddots &\ddots &  &   & \\
 & \ddots & \ddots & \ddots &   & \\
 &  & \ddots & \ddots & \ddots & \\
 &  &  & \ddots & \ddots & 1 \\
 &  &  &  & \ddots & -2 \\
 &  &  &  &  & 1
\end{bmatrix}_{T \times T-2.}
$$
From the posterior we can find the full conditionals for $\theta$ by omiting all multiplicative factors in $\pi(\boldsymbol{\eta},\theta|\mathbf{y})$ that do not depend on $\theta$.
$$
\pi(\theta|\boldsymbol{\eta},\mathbf{y}) \propto \theta^{(T-2)/2}\exp\left\{ -\theta -\frac{1}{2}\boldsymbol{\eta}^T\mathbf{Q}\boldsymbol{\eta}\right\} = \theta^{T/2-1}\exp\left\{ -\theta\left(1- \frac{1}{2}\boldsymbol{\eta}^T\mathbf{A}\boldsymbol{\eta}\right) \right\},
$$
where $\mathbf{A}$ is given by $\mathbf{A}=\frac{1}{\theta}\mathbf{Q}$. From this we can see that $\theta|\boldsymbol{\eta},\mathbf{y}$ belongs to the gamma distribution with shape parameter $\alpha=T/2$ and rate parameter $\beta=1- \frac{1}{2}\boldsymbol{\eta}^T\mathbf{A}\boldsymbol{\eta}$.
By doing the same for $\boldsymbol{\eta}$, we find the full conditional for $\boldsymbol{\eta}$,
$$
\pi(\boldsymbol{\eta}|\theta,\mathbf{y}) \propto \exp\left\{ -\frac{1}{2} \boldsymbol{\eta}^T(\mathbf{Q}+\mathbf{I})\boldsymbol{\eta}+\mathbf{y}^T\boldsymbol{\eta} \right\}.
$$
From this we can see that $\boldsymbol{\eta}|\theta,\mathbf{y}$ is Gaussian distributed with mean $\mathbf{\mu}=(\mathbf{Q}+\mathbf{I})^{-1}\mathbf{y}$ and covariance matrix $\mathbf{\Sigma}=(\mathbf{Q}+\mathbf{I})^{-1}$.
```{r}
library(Matrix)
library(matrixStats)
library(dae)

A_matrix <- function(T){
  Q <- diag(1,T,T-2)
  Q[row(Q) - col(Q) == 1] <- -2
  Q[row(Q) - col(Q) == 2] <- 1
  QQ=Q%*%t(Q)
  return(QQ)
}

#The block Gibbs sampling algorithm for the posterior distribution for theta and eta
#given y
block_Gibbs <- function(y, nsamples) {
  theta <- rep(0,nsamples)
  eta <- matrix(0,nsamples,T)
  #initial values
  theta[1] <- rgamma(1,1,1)  
  eta[1,] <- y 
  for (i in 2:nsamples){ 
    ## propose a new value for theta
    rate=1+0.5*t(eta[i-1,])%*%A%*%eta[i-1,]
    theta[i] <- rgamma(1,shape=T/2,rate=rate)
    ## propose a new vector eta with the new theta
    var=solve(A*theta[i]+I,sparse=TRUE)
    mean=var%*%y
    eta[i,] <- rmvnorm(mean, var)
      
  }
  samples=cbind(theta=theta, eta=eta) 
  return(samples)
}

n=10000
y=data$y
T <- length(y)
A <- A_matrix(T)
I=diag(1,T) #identity matrix
f_gibbs=block_Gibbs(y,n) #sample from Gibbs

#Find mean and variance from the samples of eta. Remove the first 1000 samples. 
m <- colMeans(f_gibbs[-c(1:1000),])
v <- colSds(f_gibbs[-c(1:1000),])
#Find the 95% confidence bound around the mean
m_upper <- m+qnorm(0.025)*v
m_lower <- m-qnorm(0.025)*v

#Plot the data together with the mean and the confidence bound from the samples
t=seq(1,T,1) 
plot(t,y)
lines(t,m[-1])
lines(t,m_upper[-1],col="red")
lines(t,m_lower[-1],col="red")
```
From the plot ... 
```{r}
#Histogram for theta
truehist(f_gibbs[,1])
```

## 3) 
Now we want to use the INLA scheme to approximate the posterior marginal for the hyperparameter $\theta$,  

$$
\pi(\theta|\mathbf{y}) \propto \frac{\pi(\mathbf{y}|\boldsymbol{\eta},\theta)\pi(\boldsymbol{\eta}|\theta)\pi(\theta)}{\pi(\boldsymbol{\eta}|\theta,\mathbf{y})}.
$$

In our case where the likelihood is Gaussian we dont need to approximate $\pi(\theta|\mathbf{y})$, because $\pi(\boldsymbol{\eta}|\theta,\mathbf{y})$ is also Gaussian. In point 2) we saw that $\pi(\boldsymbol{\eta}|\theta,\mathbf{y})=\mathcal{N}((\mathbf{Q}+\mathbf{I})^{-1}\mathbf{y},(\mathbf{Q}+\mathbf{I})^{-1})$. Hence, we can write the posterior marginal for $\theta$ as 
$$
\pi(\theta|\mathbf{y}) \propto \frac{\theta^{(T-2)/2}\exp\left\{ -\theta -\frac{1}{2}\left(\boldsymbol{\eta}^T(\mathbf{Q}+\mathbf{I})\boldsymbol{\eta}+ \mathbf{y}^T\mathbf{y} \right) +\mathbf{y}^T\boldsymbol{\eta} \right\}}{\left|\mathbf{Q}+\mathbf{I}\right|^{1/2}\exp\left\{ -\frac{1}{2} \boldsymbol{\eta}^T(\mathbf{Q}+\mathbf{I})\boldsymbol{\eta} - 2\boldsymbol{\eta}^T\mathbf{y} + \mathbf{y}^T(\mathbf{Q}+\mathbf{I})^{-1}\mathbf{y} \right\}} = \frac{\theta^{(T-2)/2}}{\left|\mathbf{Q}+\mathbf{I}\right|^{1/2}}\exp\left\{-\theta-\frac{1}{2}\mathbf{y}^T(\mathbf{I}-(\mathbf{Q}+\mathbf{I})^{-1})\mathbf{y} \right\}.
$$
From this we can compute the posterior marginal $\pi(\theta|\mathbf{y})$ for different $\theta$ values. We locate $K$ supporter points $\{\theta^1,\dots, \theta^K\}$ in the area of high density of $\pi(\theta|\mathbf{y})$ and compute $\pi(\theta^k|\mathbf{y})$ for each selected $\theta^k$ using the expression above. 
```{r}
pi_theta <- function(theta){
  I <- diag(T)
  det <- rep(0,length(theta))
  pi <- rep(0,length(theta))
  for (i in 1:length(theta)){
    det<-det(A*theta[i]+I)
    pi[i] <- theta[i]^(T/2-1)*exp(-theta[i])*exp(-0.5*t(y)%*%(I-solve(A*theta[i]+I))%*%y)/det^0.5
  }
  return (pi)
} 

#check the mode
mode = optimise(pi_theta,lower = 0, upper = 10, maximum = T)$maximum
#make a theta grid around mode
delta_k=0.05
theta <- seq(0,5,delta_k)
#normalize the function
const = integrate(pi_theta, lower = 0, upper = 5)$value
pi_theta_k <- pi_theta(theta)/const 
#interpolate the points
s <- smooth.spline(theta,pi_theta_k)
#Plot the marginal posterior for each theta_k and the interpolation 
plot(theta,pi_theta_k)
lines(s)

```

## 4) 
The next step is to approximate the marginal posterior for the smooth effect, $\pi(\eta_i|\mathbf{y})$. 
We approximate the integral by numerical integration as
$$
\pi(\eta_i|\mathbf{y}) = \int \pi(\eta_i|\mathbf{y},\theta)\pi(\theta|\mathbf{y})d\theta \approx
\sum_{k=1}^{K}\pi(\eta_i|\mathbf{y},\theta^k)\pi(\theta^k|\mathbf{y})\Delta_k,
$$
where $\Delta_k$ are weights that depends on the location of the support points.

```{r}
#Make a grid for eta
eta_grid <- seq(-1,1,0.01)

pi_eta_y_theta <- function(theta,eta,i=10){
  QI <- A_matrix(T)*theta+diag(1,T)
  QI_i <- QI[i,i]
  mean <- y[i]/QI_i
  var <- 1/QI_i
  pi <- dnorm(eta,mean, sqrt(var))
  return(pi)
}

#Approximate the integral via numerical integration 
pi_eta_y <- function(theta,i=10){
  sum=rep(0,length(eta_grid))
  for (k in 1:length(theta)){
    sum = sum + (pi_eta_y_theta(theta[k],eta_grid)*pi_theta_k[k]*delta_k)
  }
  return (sum)
}

#Approximate the marginal posterior for eta_10. 
pi_eta_10 <- pi_eta_y(theta,i=10)
plot(eta_grid,pi_eta_10)

#Histogram for eta_10 with Gibbs
truehist(f_gibbs[-c(1:1000),11])
```


## 5) 
Now we want to compare our results above with the results the inla() function gives. 
```{r}
library("INLA")
x=(1:20)
df=data.frame(y=data$y,x=x)

# specify the prior
my.hyper <- list(theta = list(prior="log.gamma", param=c(1,1)))
# specify the linear predictor
formula <- y ~ -1 + f(x, model = "rw2", hyper = my.hyper, constr=FALSE)
#Fit the model
result <- inla(formula=formula, family="gaussian", data=df,verbose = FALSE, control.family = list(hyper = list(prec=list(initial=0,fixed=TRUE)))) 

plot(result$summary.random$x$mean)
plot(result$marginals.hyperpar$`Precision for x`,xlim=c(0,6))
plot(result$marginals.random$x$index.10, xlim=c(-2,2))
summary(result)
```

