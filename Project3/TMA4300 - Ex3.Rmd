---
title: "Computer Intensive Statistical Methods - Exercise 3"
author: "Vebj√∏rn Rekkebo, Camilla Karlsen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

# Problem A: Comparing $AR(2)$ parameter estimators using resampling of residuals
```{r}
#Fetch data
source("probAhelp.R")
source("probAdata.R")
```
In this exercise we analyse a dataset containing a sequence of length T=100 of a non-Gaussian time series. We consider an AR(2) model which is specified by the relation
$$
  x_t = \beta_1x_{t-1} + \beta_2x_{t-2} + e_t,
$$
where $e_t$ are iid random variables with zero mean and constant variance. The least sum of squared residuals (LS) and least sum of absolute residuals (LA) are obtained by minimising the following loss functions with respect to $\beta$:
$$
Q_{LS}(\boldsymbol{x}) = \sum_{t=3}^T (x_t - \beta_1x_{t-1} - \beta_2x_{t-2})^2
$$
$$
Q_{LA}(\boldsymbol{x}) = \sum_{t=3}^T |x_t - \beta_1x_{t-1} - \beta_2x_{t-2}|.
$$
We denote the minimisers by $\hat{\boldsymbol{\beta}}_{LS}$ and $\hat{\boldsymbol{\beta}}_{LA}$, with corresponding residuals $\hat{e}_t=x_t - \hat\beta_1x_{t-1} - \hat\beta_2x_{t-2}$ for $t=3,\dots,T$. We let $\bar{e}_t$ denote the mean of the residuals and center them by defining $\epsilon_t=\hat e_t-\bar{e}_t$. All of the computations and estimations in this problem are done using both LS and LA residuals.

```{r}
x0 <- data3A$x
T <- length(x0)
#Compute estimates for beta
betahat <- ARp.beta.est(x0,2)
#Find the corresponding residuals
res_LS <- ARp.resid(x0,betahat$LS)
res_LA <- ARp.resid(x0,betahat$LA)
```
## 1) 
We now generate $B=1500$ bootstrap samples of the residuals, each containing $T$ elements randomly picked from $\hat{\boldsymbol{\epsilon}}$ with replacement. From the new residuals and $\hat{\boldsymbol{\beta}}$ we resample the time-series once for each sample. Here we use initial values $x_{k}$ and $x_{k+1}$ where $k$ is chosen randomly from the set $\{1,2,\dots,T-1\}$. 

```{r}
B <- 1500
n_res <- length(res_LS)
#Bootstrap x from LS residuals
xb_LS <- matrix(NA,T,B)
for (b in 1:B){
  res <- sample(res_LS,n_res,replace=TRUE)
  x <- ARp.filter(x0[rep(sample(99,1),2)+c(0,1)],betahat$LS,res)
  xb_LS[,b] <- x
}

#Bootstrap x from LA residuals
xb_LA <- matrix(NA,T,B)
for (b in 1:B){
  res <- sample(res_LA,n_res,replace=TRUE)
  x <- ARp.filter(x0[rep(sample(99,1),2)+c(0,1)],betahat$LA,res)
  xb_LA[,b] <- x
}
```
From the new time series samples we now estimate $B$ new coeffients $\hat{\boldsymbol{\beta}}_{b}$, one for each time series.
```{r}
#Compute beta_LS from bootstrapped time series
betahatb_LS <- matrix(NA,2,B)
for (b in 1:B){
  betahatb_LS[,b] <- ARp.beta.est(xb_LS[,b],2)$LS
}
#Compute beta_LA from bootstrapped time series
betahatb_LA <- matrix(NA,2,B)
for (b in 1:B){
  betahatb_LA[,b] <- ARp.beta.est(xb_LA[,b],2)$LA
}
```
With a sample of coeffients we can easily estimate the variance and bias of both values for $\hat{\boldsymbol{\beta}}$. The variance is estimated by the formula
$$
\widehat{\text{Var}(\hat{\boldsymbol{\beta}})} = \frac{1}{B-1}\sum_{b=1}^B (\hat{\boldsymbol{\beta}}-\bar{\boldsymbol{\beta}})^2
$$
where $\bar{\boldsymbol{\beta}}=\sum_{b=1}^B \hat{\boldsymbol{\beta_b}}$ is the average of over the bootstrap sample. To estimate the bias we apply the plug-in principle to get the formula 
$$
\widehat{\text{bias}(\hat{\boldsymbol{\beta}})} = \bar{\boldsymbol{\beta}}-\hat{\boldsymbol{\beta}}.
$$

```{r}
library(matrixStats)
#Estimate variance and bias of beta_LS
rowVars(betahatb_LS)
rowMeans(betahatb_LS) - betahat$LS
```
```{r}
#Estimate variance and bias of beta_LA
rowVars(betahatb_LA)
rowMeans(betahatb_LA) - betahat$LA
```
We observe that the estimates for both the variance and bias are much smaller for $\hat{\boldsymbol{\beta}}_{LA}$ than for $\hat{\boldsymbol{\beta}}_{LS}$. This shows that the LS estimator is not optimal for this problem.

## 2) 
Next we want to compute a 95% prediction interval for $x_{101}$ based on both estimators. We start by estimating the residuals corresponding to the bootstrapped time series and parameter estimates from part 1).
```{r}
#########Usikker her!
#Estimate LS residuals from bootstap samples of x and beta
resb_LS <- matrix(NA,n_res,B)
for (b in 1:B){
  resb_LS[,b] <- ARp.resid(xb_LS[,b],betahatb_LS[,b])
}
#Estimate LA residuals from bootstap samples of x and beta
resb_LA <- matrix(NA,n_res,B)
for (b in 1:B){
  resb_LA[,b] <- ARp.resid(xb_LA[,b],betahatb_LA[,b])
}
```
This results in $B\times (T-2)=147000$ residual values in total. Since the residuals are iid we can use all of them in a sample for $\epsilon_{101}$. Thus, using the formula 
$$
x_{101} = \hat\beta_1x_{100} + \hat\beta_2x_{99} + \hat\epsilon_t,
$$
we get 147000 simulations for $x_{101}$. From this sample we find the quantiles corresponding to a 95% prediction interval.
```{r}
#Simulate x101 for each residual value
x101_LS <- betahat$LS[1]*x0[T] + betahat$LS[2]*x0[T-1] + as.vector(resb_LS)
x101_LA <- betahat$LA[1]*x0[T] + betahat$LA[2]*x0[T-1] + as.vector(resb_LA)
#95% prediction intervals based on quantiles of simulated x101
#LS:
c(quantile(x101_LS,0.025), quantile(x101_LS,.975))
#LA:
c(quantile(x101_LA,0.025), quantile(x101_LA,.975))
```


# Problem B: Permutation test
```{r}
#Fetch data
bilirubin <- read.table("bilirubin.txt",header=T)
```
## 1)
```{r}
#Boxplot
boxplot(log(meas)~pers,data=bilirubin,xlab="Person",ylab="log(Concentration)")
```

```{r}
#Fit linear model
fit <- lm(log(meas)~pers,data=bilirubin)
#Find F-value
Fval <- summary(fit)$fstatistic[1]
```
## 2)
```{r}
#Function to compute F-value of random permutation.
#NB! y0=log(meas)
permTest <- function(x0,y0){
  n <- length(x0)
  x <- sample(x0,n)
  return(as.numeric(summary(lm(y0~x))$fstatistic[1]))
}
```
## 3)
```{r}
#Sample 999 values of F
n_perms <- 999
F_sample <- rep(NA,n_perms)
for (i in 1:n_perms){
  F_sample[i] <- permTest(bilirubin$pers,log(bilirubin$meas))
}
#Compute p-value using the sample of F-values
pval <- sum(F_sample > Fval)/n_perms
```

# Problem C: The EM-algorithm and bootstrapping 

We let $x_1,\dots,x_n$ and $y_1,\dots,y_n$ be independent random variables, where the $x_i$'s and $y_i$'s have an exponential distribution with intensity respectively $\lambda_0$ and $\lambda_1$. We assume that we do not observe $x_1,\dots,x_n$ and $y_1,\dots,y_n$ directly, but instead observe $z_i=\max(x_i,y_i)$ and $u_i=I(x_i\geq y_i)$ for $i=1,\dots,n$, where $I(\cdot)$ is the indicator function. Based on the observed $(z_i,u_i),i=1,\dots,n$ we will us the EM algorithm to find the maximum likelhood estimates for $\boldsymbol{\theta} = (\lambda_0,\lambda_1)$.

## 1)
We start by finding the log likelihood function for the complete data $(x_i,y_i),i=1,\dots,n$.  Since $x_i$ and $y_i$ are assumed independent do we have 
$$
f(\mathbf{x},\mathbf{y}|\boldsymbol{\theta})=\prod_{i=1}^n f(x_i|\lambda_0)f(y_i|\lambda_1) = \prod_{i=1}^n \lambda_0\lambda_1 \exp(-\lambda_0x_i) \exp(-\lambda_1y_i)
=(\lambda_0 \lambda_1)^n \exp\left(-\lambda_0 \sum_{i=1}^n x_i\right) \exp\left(-\lambda_1 \sum_{i=1}^n y_i\right).
$$
Thus we get the log likelihood function 
$$
\ln f(\mathbf{x},\mathbf{y}|\boldsymbol{\theta})=n\ln(\lambda_0) + n\ln(\lambda_1) - \lambda_0\sum_{i=1}^nx_i - \lambda_1\sum_{i=1}^ny_i.
$$
The EM algorithm itereates between performing an expectation (E) step and a maximization (M) step. In the expecation step, a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters is calculated. In the maximization step, the expected log-likelihood found in the E-step is maximazed.  

We start with the E-step and compute the conditional expectation 
$$
Q(\boldsymbol{\theta})=Q\left(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)}\right)
=E\left[\ln f(\mathbf{x},\mathbf{y}|\boldsymbol{\theta})|\mathbf{z},\mathbf{u},\boldsymbol{\theta}^{(t)}\right]=n\ln(\lambda_0) + n\ln(\lambda_1) - \lambda_0\sum_{i=1}^nE\left[x_i|z_i,u_i,\boldsymbol{\theta}^{(t)}\right] - \lambda_1\sum_{i=1}^nE\left[y_i|z_i,u_i,\boldsymbol{\theta}^{(t)}\right]
$$. 

## 2) 

## 3) 

## 4) 
