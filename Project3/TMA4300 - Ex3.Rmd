---
title: "Computer Intensive Statistical Methods - Exercise 3"
author: "Vebjørn Rekkebo, Camilla Karlsen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

# Problem A: Comparing $AR(2)$ parameter estimators using resampling of residuals

In this exercise we analyse a dataset containing a sequence of length T=100 of a non-Gaussian time series.
```{r}
#Fetch data
source("probAhelp.R")
source("probAdata.R")
```
We consider an AR(2) model which is specified by the relation
$$
  x_t = \beta_1x_{t-1} + \beta_2x_{t-2} + e_t,
$$
where $e_t$ are iid random variables with zero mean and constant variance. The least sum of squared residuals (LS) and least sum of absolute residuals (LA) are obtained by minimising the following loss functions with respect to $\beta$:
$$
Q_{LS}(\boldsymbol{x}) = \sum_{t=3}^T (x_t - \beta_1x_{t-1} - \beta_2x_{t-2})^2
$$
$$
Q_{LA}(\boldsymbol{x}) = \sum_{t=3}^T |x_t - \beta_1x_{t-1} - \beta_2x_{t-2}|.
$$
We denote the minimisers by $\hat{\boldsymbol{\beta}}_{LS}$ and $\hat{\boldsymbol{\beta}}_{LA}$, with corresponding residuals $\hat{e}_t=x_t - \hat\beta_1x_{t-1} - \hat\beta_2x_{t-2}$ for $t=3,\dots,T$. We let $\bar{e}_t$ denote the mean of the residuals and center them by defining $\hat\epsilon_t=\hat e_t-\bar{e}_t$. All of the computations and estimations in this problem are done using both LS and LA residuals.

```{r}
x0 <- data3A$x
T <- length(x0)
#Compute estimates for beta
betahat <- ARp.beta.est(x0,2)
#Find the corresponding residuals
res_LS <- ARp.resid(x0,betahat$LS)
res_LA <- ARp.resid(x0,betahat$LA)
```
## 1) 
We now generate $B=1500$ bootstrap samples of the residuals, each containing $T$ elements randomly picked from $\hat{\boldsymbol{\epsilon}}$ with replacement. From the new residuals and $\hat{\boldsymbol{\beta}}$ we resample the time series once for each sample. Here we use initial values $x_{k}$ and $x_{k+1}$ where $k$ is chosen randomly from the set $\{1,2,\dots,T-1\}$ for every new times series. 

```{r}
B <- 1500
n_res <- length(res_LS)
#Bootstrap x from LS residuals
xb_LS <- matrix(NA,T,B)
for (b in 1:B){
  res <- sample(res_LS,n_res,replace=TRUE)
  x <- ARp.filter(x0[rep(sample(99,1),2)+c(0,1)],betahat$LS,res)
  xb_LS[,b] <- x
}

#Bootstrap x from LA residuals
xb_LA <- matrix(NA,T,B)
for (b in 1:B){
  res <- sample(res_LA,n_res,replace=TRUE)
  x <- ARp.filter(x0[rep(sample(99,1),2)+c(0,1)],betahat$LA,res)
  xb_LA[,b] <- x
}
```
From the new time series sample we now estimate $B$ new coeffients $\hat{\boldsymbol{\beta}}_{b}$, one for each time series.
```{r}
#Compute beta_LS from bootstrapped time series
betahatb_LS <- matrix(NA,2,B)
for (b in 1:B){
  betahatb_LS[,b] <- ARp.beta.est(xb_LS[,b],2)$LS
}
#Compute beta_LA from bootstrapped time series
betahatb_LA <- matrix(NA,2,B)
for (b in 1:B){
  betahatb_LA[,b] <- ARp.beta.est(xb_LA[,b],2)$LA
}
```
With a sample of coeffients we can easily estimate the variance and bias of both values for $\hat{\boldsymbol{\beta}}$. The variance is estimated by the formula
$$
\widehat{\text{Var}(\hat{\boldsymbol{\beta}})} = \frac{1}{B-1}\sum_{b=1}^B (\hat{\boldsymbol{\beta}}-\bar{\boldsymbol{\beta}})^2
$$
where $\bar{\boldsymbol{\beta}}=\sum_{b=1}^B \hat{\boldsymbol{\beta}}_b$ is the average of over the bootstrap sample. 
To estimate the bias we apply the plug-in principle to get the expression 
$$
\widehat{\text{bias}(\hat{\boldsymbol{\beta}})} = \bar{\boldsymbol{\beta}}-\hat{\boldsymbol{\beta}}.
$$

```{r}
library(matrixStats)
#LS
#Estimated variance of beta1,beta2:
rowVars(betahatb_LS)
#Estimated bias of beta1,beta2:
rowMeans(betahatb_LS) - betahat$LS
```
```{r}
#LA
#Estimated variance of beta1,beta2:
rowVars(betahatb_LA)
#Estimated bias of beta1,beta2:
rowMeans(betahatb_LA) - betahat$LA
```
We observe that the estimates for both the variance and bias are much smaller for $\hat{\boldsymbol{\beta}}_{LA}$ than for $\hat{\boldsymbol{\beta}}_{LS}$. This shows that the LS estimator is not optimal for this problem.

## 2) 
Next we want to compute a 95% prediction interval for $x_{101}$ based on both estimators. We start by estimating the residuals corresponding to the bootstrapped time series and parameter estimates from part 1).
```{r}
#Estimate LS residuals from bootstap samples of x and beta
resb_LS <- matrix(NA,n_res,B)
for (b in 1:B){
  resb_LS[,b] <- ARp.resid(xb_LS[,b],betahatb_LS[,b])
}
#Estimate LA residuals from bootstap samples of x and beta
resb_LA <- matrix(NA,n_res,B)
for (b in 1:B){
  resb_LA[,b] <- ARp.resid(xb_LA[,b],betahatb_LA[,b])
}
```
This results in $B\times (T-2)=147000$ residual values in total. Since the residuals are iid we can use all of them in a sample for $\epsilon_{101}$. Thus, using the formula 
$$
x_{101} = \hat\beta_1x_{100} + \hat\beta_2x_{99} + \epsilon_{101},
$$
we get 147000 simulated values for $x_{101}$. From this sample we find the quantiles corresponding to a 95% prediction interval.
```{r}
#Simulate x101 for each residual value
x101_LS <- betahat$LS[1]*x0[T] + betahat$LS[2]*x0[T-1] + as.vector(resb_LS)
x101_LA <- betahat$LA[1]*x0[T] + betahat$LA[2]*x0[T-1] + as.vector(resb_LA)
#95% prediction intervals based on quantiles of simulated x101
#LS:
c(quantile(x101_LS,0.025), quantile(x101_LS,.975))
#LA:
c(quantile(x101_LA,0.025), quantile(x101_LA,.975))
```
The two prediction intervals are fairly similar and tell us that the change is in most steps expected to be smaller than approximately 8.

# Problem B: Permutation test

In this problem we consider measurements of the concentration of bilirubin in blood samples $\mathbf{Y}$ taken from three different men. The total number of measurements is $n=29$, distributed on $n_1=11$, $n_2=10$, $n_3=8$.

```{r}
#Fetch data
bilirubin <- read.table("bilirubin.txt",header=T)
```
## 1)
We first make a boxplot of $\log(Y_{i})$, $i=1,2,3$, to compare each person.
```{r}
#Boxplot
boxplot(log(meas)~pers,data=bilirubin,xlab="Person",ylab="log(Concentration)")
```

We see that person 1 and person 2 have similar median values, but person 1 has much larger variability in his values. Person 3 has much larger median value than the two others as well as high variability in measurements.

We fit the linear regression model 
$$
\log(Y_{ij})=\beta_i+\epsilon_{ij}, \text{ with      } i=1,2,3 \text{ and } j=1,2,\ldots,n_i
$$
where $\epsilon_{ij}\sim\mathcal{N}(0,\sigma^2)$ are iid. The F-test tests the hypothesis $\beta_1=\beta_2=\beta_3$.
```{r}
#Fit linear model
fit <- lm(log(meas)~pers,data=bilirubin)
#Find F-value
Fval <- summary(fit)$fstatistic[1]
```
We get $F_0=3.67$ which corresponds to the p-value 0.039. Thus we reject the null hypothesis with significance level 0.05 and conclude that the three men have different concentrations of bilirubin in their blood.

## 2)
We now write a function `permTest` to perform a permutation test. The function shuffles the measurements between the three men, then fits a similar linear regression model as in part 1), and returns the corresonding F-statistic.
```{r}
#Function to compute F-value of random permutation.
#NB! y0=log(meas)
permTest <- function(x0,y0){
  n <- length(x0)
  x <- sample(x0,n)
  return(as.numeric(summary(lm(y0~x))$fstatistic[1]))
}
```
## 3)
To perform the permutation test we make a sample of size 999 of the F-statistic. We then compute the p-value as $\frac{1}{999}\sum_{i=1}^{999}I(F_i>F_0)$ where $I$ denotes the indicator function and $F_i$ is the F-statistic corresponding to permutation $i$.
```{r}
set.seed(0)
#Sample 999 values of F
n_perms <- 999
F_sample <- rep(NA,n_perms)
for (i in 1:n_perms){
  F_sample[i] <- permTest(bilirubin$pers,log(bilirubin$meas))
}
#Compute p-value using the sample of F-values
pval <- sum(F_sample > Fval)/n_perms
pval
```
The reported p-value is `r round(pval,4)` which is very close to the original p-value. The permutation test is a more robust test as it does not require any model assumptions. However, getting such similar results indicate that the model assumptions of the linear regression model may be satisfied as well.

# Problem C: The EM-algorithm and bootstrapping 

We let $x_1,\dots,x_n$ and $y_1,\dots,y_n$ be independent random variables, where the $x_i$'s and $y_i$'s have an exponential distribution with intensity respectively $\lambda_0$ and $\lambda_1$. We assume that we do not observe $x_1,\dots,x_n$ and $y_1,\dots,y_n$ directly, but instead observe $z_i=\max(x_i,y_i)$ and $u_i=I(x_i\geq y_i)$ for $i=1,\dots,n$, where $I(\cdot)$ is the indicator function. Based on the observed $(z_i,u_i),i=1,\dots,n$ we will us the EM algorithm to find the maximum likelhood estimates for $\boldsymbol{\theta} = (\lambda_0,\lambda_1)$.

## 1)
We start by finding the log likelihood function for the complete data $(x_i,y_i),i=1,\dots,n$.  Since $x_i$ and $y_i$ are assumed independent do we have 
$$
f(\mathbf{x},\mathbf{y}\lvert  \boldsymbol{\theta})=\prod_{i=1}^n f(x_i\lvert  \lambda_0)f(y_i\lvert  \lambda_1) = \prod_{i=1}^n \lambda_0\lambda_1 \exp(-\lambda_0x_i) \exp(-\lambda_1y_i)
=(\lambda_0 \lambda_1)^n \exp\left(-\lambda_0 \sum_{i=1}^n x_i\right) \exp\left(-\lambda_1 \sum_{i=1}^n y_i\right).
$$
Thus we get the log likelihood function 
$$
\ln f(\mathbf{x},\mathbf{y}\lvert  \boldsymbol{\theta})=n\ln(\lambda_0) + n\ln(\lambda_1) - \lambda_0\sum_{i=1}^nx_i - \lambda_1\sum_{i=1}^ny_i.
$$

The EM algorithm iterates between performing an expectation (E) step and a maximization (M) step. In the expecation step, a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters is calculated, $Q(\boldsymbol{\theta})=Q\left(\boldsymbol{\theta}\lvert  \boldsymbol{\theta}^{(t)}\right)$. The function $Q(\boldsymbol{\theta})$ is then maximized in the M-step to find the maximum likelihood estimates of $\boldsymbol{\theta}$. These parameter-estimates are then used in the next E step. The steps are repeated until convergence. 

We start by looking at the E-step and compute the conditional expectation 
$$
Q(\boldsymbol{\theta})
=E\left[\ln f(\mathbf{x},\mathbf{y}\lvert  \boldsymbol{\theta})\lvert  \mathbf{z},\mathbf{u},\boldsymbol{\theta}^{(t)}\right]=n\ln(\lambda_0) + n\ln(\lambda_1) - \lambda_0\sum_{i=1}^nE\left[x_i\lvert  z_i,u_i,\boldsymbol{\theta}^{(t)}\right] - \lambda_1\sum_{i=1}^nE\left[y_i\lvert  z_i,u_i,\boldsymbol{\theta}^{(t)}\right]
$$. 

Thus, we need to find the $E\left[x_i\lvert  z_i,u_i,\boldsymbol{\theta}^{(t)}\right]$ and $E\left[y_i\lvert   z_i,u_i,\boldsymbol{\theta}^{(t)}\right]$. The conditional probabilities of $x_i$ and $y_i$ given $z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}$ are given as. 

$$
f(x_i \lvert z_i, u_i, \boldsymbol{\theta}^{(t)}) 
= \begin{cases}\frac{\lambda_0^{(t)} \exp ( -\lambda_0^{(t)} x_i )}{1 - \exp (-\lambda_0^{(t)} z_i )}, \quad &u_i = 0, \\
z_i, \quad &u_i = 1,
\end{cases}
$$
$$
f(y_i \lvert z_i, u_i, \boldsymbol{\theta}^{(t)}) 
= \begin{cases} z_i, \quad &u_i = 0, \\
\frac{\lambda_1^{(t)} \exp ( -\lambda_1^{(t)} y_i )}{1 - \exp (-\lambda_1^{(t)} z_i )}, \quad &u_i = 1. \\
\end{cases}
$$
Then we compute the expectations as 
$$
E\left[x_i\lvert  z_i,u_i,\boldsymbol{\theta}^{(t)}\right] 
= u_iz_i + (1-u_i)\int_0^{z_i} x_i \frac{\lambda_0^{(t)}\exp(-\lambda_0^{(t)}x_i)}{1-\exp(\lambda_0^{(t)}z_i)}
= u_iz_i + (1-u_i)\left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i)-1}\right),
$$
$$
E\left[y_i\lvert  z_i,u_i,\boldsymbol{\theta}^{(t)}\right] 
= (1-u_i)z_i + u_i\int_0^{z_i} y_i \frac{\lambda_1^{(t)}\exp(-\lambda_1^{(t)}y_i)}{1-\exp(\lambda_1^{(t)}z_i)}
= (1-u_i)z_i + u_i\left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i)-1}\right).
$$
By inserting the expectations above in the expression of the conditional expectation, we get
$$
Q(\boldsymbol{\theta})=n\ln(\lambda_0) + n\ln(\lambda_1) - \lambda_0\sum_{i=1}^n\left[u_iz_i + (1-u_i)\left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i)-1}\right)\right] - \lambda_1\sum_{i=1}^n\left[ (1-u_i)z_i + u_i\left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i)-1}\right)\right].
$$

## 2) 
In the M-step we want to maximize the function $Q(\boldsymbol{\theta})$, found on the E-step, to find the maximum likelihood estimates for $\boldsymbol{\theta}=(\lambda_0,\lambda_1)$. 

By using
$$
\frac{dQ}{d\lambda_0}=0 \quad \frac{dQ}{d\lambda_1}=0,
$$
we find the recursion
$$
\lambda_0^{(t+1)} = \frac{n}{\sum_{i=1}^n\left[u_iz_i + (1-u_i)\left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i)-1}\right)\right]},
$$
$$
\lambda_1^{(t+1)} = \frac{n}{\sum_{i=1}^n\left[ (1-u_i)z_i + u_i\left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i)-1}\right)\right]}.
$$

We have implemented the recursion to find the maximum likelihood estimates for $(\lambda_0,\lambda_1)$ when the data, $z$ and $u$, are given. 

```{r}
z <- read.table("https://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/ex3-additionalFiles/z.txt")[,1]
u <- read.table("https://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/ex3-additionalFiles/u.txt")[,1]
n <- length(u)
ntimes <- 50


recursion <- function(lam,z,u){
  lambda_0 <- lam[1]
  lambda_1 <- lam[2]
  sum_0 <- u%*%z + (1-u)%*%(1/lambda_0-z/(exp(lambda_0*z)-1))
  sum_1 <- (1-u)%*%z + u%*%(1/lambda_1-z/(exp(lambda_1*z)-1))
  lambda_0 <- n/(sum_0)
  lambda_1 <- n/(sum_1)
  return(cbind(lambda_0,lambda_1))
}

lambda <- matrix(0,nrow=ntimes,ncol=2)
lambda[1,] <- cbind(1,1)
for (i in 2:ntimes){
  lambda[i,] <- recursion(lambda[i-1,],z,u)
}

#the maximum likelihood estimates for (lambda_0,lambda_1)
lambda_mle <- lambda[ntimes,]
cat("lambda_mle: ",lambda_mle)

#plot lambda as a function of steps
plot(1:ntimes,lambda[,1],type="l", col="red",ylim=c(0,10),ylab="lambda",xlab="number of iterations")
lines(lambda[,2])
```

From the plot can we see that the algorithm converges after a few iterations, and that the maximum likelihood estimates for $(\lambda_0,\lambda_1)$ is $(3.47,9.35)$. Now we want to visualise the convergence of the algorithm with the norm $||(\lambda_0^{(t)}, \lambda_1^{(t)}) - (\lambda_0^{(t-1)}, \lambda_1^{(t-1)}) ||$.

```{r}
#Convergence plot
#find difference in each update for lambda_0 and lambda_1
diff <- diff(lambda) 
#find the norm of the update
norm <- sqrt(diff[,1]+diff[,2]) 
#plot the norm as a function of steps
plot(1:(ntimes-1),norm, type="l", xlab="number of iterations")
```

The convergence plot shows the same as mentioned above, the algorithm converges very quickly. 

## 3) 
Now we want to use bootstrapping to estimate the standard deviations and the biases of each of $\hat{\lambda}_0$ and $\hat{\lambda}_1$ and to estimate the correlation $\text{Corr}[\hat{\lambda}_0,\hat{\lambda}_1]$. The pseudocode for our bootstrap algorithm is presented belov. We are interested in $\boldsymbol{\theta}^*_b(1),\dots,\boldsymbol{\theta}^*_b(B)$, which is the basis for estimating the distribution of $\hat{\boldsymbol{\theta}}$.
$$
\begin{align*}
    & \textit{for b in } 1,2,\dots,B:\\
    & \quad z_b^* \leftarrow \textit{sample with replacement from z}\\
    & \quad u_b^* \leftarrow \textit{sample with replacement from u}\\
    & \quad \textit{compute } \boldsymbol{\theta}^*_b=(\lambda_{0,b}^*,\lambda_{1,b}^*) \textit{by running the EM-algorithm with }z_b^* \textit{and } u_b^*\\
    & \quad \boldsymbol{\theta}^*_b[b]=\boldsymbol{\theta}^*_b
\end{align*}
$$

```{r}
B=10000

EM<-function(z,u){
  norm <- 1
  lambda <- runif(2)
  while (norm >1e-12){
    lambda_old <- lambda
    lambda <- recursion(lambda,z,u)
    norm <- sum((lambda-lambda_old)^2)
  }
  return (lambda)
}

lambda_b <- matrix(0,nrow=B,ncol=2)
for (b in 1:B){
  z_b <- sample(z,replace=T)
  u_b <- sample(u,replace=T)
  lambda_b[b,] <- EM(z_b,u_b)
}

#plot lambda_b as a function of steps
plot(1:B,lambda_b[,1],type="l", col="red",ylim=c(0,10),ylab="lambda_b",xlab="number of iterations")
lines(lambda_b[,2])

mean_lambda <- colMeans(lambda_b)
sd_lambda <- sqrt(1/(B-1)*colSums((lambda_b-mean_lambda)^2)) 
bias <- mean_lambda - lambda_mle
corr <- cor(lambda_b)[1, 2] 
cat(" Mean: ",mean_lambda,"\n","Standard deviation: ",sd_lambda,"\n","Bias: ",bias,"\n","Corr: ",corr,"\n")
```
We observe that the mean $(\bar{\lambda}_0^*, \bar{\lambda}_1^*)=(3.91,6.49)$ of the bootstrap samples is a bit different from the maximum likelihood estimates $\lambda_{0}}, \lambda_{1}=(3.47,9.35)$, espesially for $\lambda_1$.
MLE eller bias corrected estimates? MLE siden variancen er større i bootstrap. 
As expected, since $x_i$ and $y_i$ are independent random variables, is the correlation $\text{Corr}[\hat{\lambda}_0,\hat{\lambda}_1]$ very small.


## 4) 
