---
title: "Computer Intensive Statistical Methods - Exercise 3"
author: "Vebjørn Rekkebo, Camilla Karlsen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

# Problem A: Comparing $AR(2)$ parameter estimators using resampling of residuals

## 1) 
```{r}
library(matrixStats)
source("probAhelp.R")
source("probAdata.R")
```

```{r}
x0 <- data3A$x
n <- length(x0)
betahat <- ARp.beta.est(x0,2)
res_LS <- ARp.resid(x0,betahat$LS)
res_LA <- ARp.resid(x0,betahat$LA)
```

```{r}
B <- 1500
n_res <- length(res_LS)
#Bootstrap x for LS
xb_LS <- matrix(NA,n,B)
for (b in 1:B){
  res <- sample(res_LS,n_res,replace=TRUE)
  x <- ARp.filter(x0[rep(sample(99,1),2)+c(0,1)],betahat$LS,res)
  xb_LS[,b] <- x
}

#Bootstrap x for LA
xb_LA <- matrix(NA,n,B)
for (b in 1:B){
  res <- sample(res_LA,n_res,replace=TRUE)
  x <- ARp.filter(x0[rep(sample(99,1),2)+c(0,1)],betahat$LA,res)
  xb_LA[,b] <- x
}

#Compute beta_LS from bootstrap sample
betahatb_LS <- matrix(NA,2,B)
for (b in 1:B){
  betahatb_LS[,b] <- ARp.beta.est(xb_LS[,b],2)$LS
}
#Estimate variance of beta_LS
var_beta_LS <- rowVars(betahatb_LS)
bias_beta_LS <- rowMeans(betahatb_LS) - betahat$LS

#Compute beta_LA from bootstrap sample
betahatb_LA <- matrix(NA,2,B)
for (b in 1:B){
  betahatb_LA[,b] <- ARp.beta.est(xb_LA[,b],2)$LA
}
#Estimate variance of beta_LA
var_beta_LA <- rowVars(betahatb_LA)
bias_beta_LA <- rowMeans(betahatb_LA) - betahat$LA
```

## 2) 
```{r}
#########Usikker her!
#Estimate LS residuals from bootstap samples of x and beta
resb_LS <- matrix(NA,n_res,B)
for (b in 1:B){
  resb_LS[,b] <- ARp.resid(xb_LS[,b],betahatb_LS[,b])
}
#Estimate LA residuals from bootstap samples of x and beta
resb_LA <- matrix(NA,n_res,B)
for (b in 1:B){
  resb_LA[,b] <- ARp.resid(xb_LA[,b],betahatb_LA[,b])
}

##USIKKER på res her!
#Simulate x101 for each residual value
x101_LS <- betahat$LS[1]*x0[n] + betahat$LS[2]*x0[n-1] + as.vector(resb_LS)
x101_LA <- betahat$LA[1]*x0[n] + betahat$LA[2]*x0[n-1] + as.vector(resb_LA)
#Prediction intervals based on quantiles of simulated x101
pred_LS <- c(quantile(x101_LS,0.025), quantile(x101_LS,.975))
pred_LA <- c(quantile(x101_LA,0.025), quantile(x101_LA,.975))

```

# Problem B: Permutation test

## 1) 

## 2)

## 3)

# Problem C: The EM-algorithm and bootstrapping 

We let $x_1,\dots,x_n$ and $y_1,\dots,y_n$ be independent random variables, where the $x_i$'s and $y_i$'s have an exponential distribution with intensity respectively $\lambda_0$ and $\lambda_1$. We assume that we do not observe $x_1,\dots,x_n$ and $y_1,\dots,y_n$ directly, but instead observe $z_i=\max(x_i,y_i)$ and $u_i=I(x_i\geq y_i)$ for $i=1,\dots,n$, where $I(\cdot)$ is the indicator function. Based on the observed $(z_i,u_i),i=1,\dots,n$ we will us the EM algorithm to find the maximum likelhood estimates for $\boldsymbol{\theta} = (\lambda_0,\lambda_1)$.

## 1)
We start by finding the log likelihood function for the complete data $(x_i,y_i),i=1,\dots,n$.  Since $x_i$ and $y_i$ are assumed independent do we have 
$$
f(\mathbf{x},\mathbf{y}|\boldsymbol{\theta})=\prod_{i=1}^n f(x_i|\lambda_0)f(y_i|\lambda_1) = \prod_{i=1}^n \lambda_0\lambda_1 \exp(-\lambda_0x_i) \exp(-\lambda_1y_i)
=(\lambda_0 \lambda_1)^n \exp\left(-\lambda_0 \sum_{i=1}^n x_i\right) \exp\left(-\lambda_1 \sum_{i=1}^n y_i\right).
$$
Thus we get the log likelihood function 
$$
\ln f(\mathbf{x},\mathbf{y}|\boldsymbol{\theta})=n\ln(\lambda_0) + n\ln(\lambda_1) - \lambda_0\sum_{i=1}^nx_i - \lambda_1\sum_{i=1}^ny_i.
$$
The EM algorithm itereates between performing an expectation (E) step and a maximization (M) step. In the expecation step, a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters is calculated. In the maximization step, the expected log-likelihood found in the E-step is maximazed.  

We start with the E-step and compute the conditional expectation 
$$
Q(\boldsymbol{\theta})=Q\left(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)}\right)
=E\left[\ln f(\mathbf{x},\mathbf{y}|\boldsymbol{\theta})|\mathbf{z},\mathbf{u},\boldsymbol{\theta}^{(t)}\right]=n\ln(\lambda_0) + n\ln(\lambda_1) - \lambda_0\sum_{i=1}^nE\left[x_i|z_i,u_i,\boldsymbol{\theta}^{(t)}\right] - \lambda_1\sum_{i=1}^nE\left[y_i|z_i,u_i,\boldsymbol{\theta}^{(t)}\right]
$$. 

## 2) 

## 3) 

## 4) 
